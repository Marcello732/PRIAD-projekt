{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8fc110",
   "metadata": {},
   "source": [
    "## Wstęp\n",
    " \n",
    "Celem niniejszego sprawozdania jest analiza oraz przygotowanie modelu uczenia maszynowego do prognozowania cen mieszkań w Polsce na podstawie publicznego zbioru danych “Apartment Prices in Poland” udostępnionego na platformie Kaggle. Zbiór zawiera oferty dotyczące rynku mieszkaniowego (ogłoszenia) w największych miastach w Polsce.\n",
    "\n",
    "Zbiór znajduje się tutaj: https://www.kaggle.com/datasets/krzysztofjamroz/apartment-prices-in-poland/data\n",
    "\n",
    "Dane obejmują okres od sierpnia 2023 do stycznia 2024 (miesięczne wycinki/aktualizacje), co pozwala obserwować zmienność rynku w krótkim horyzoncie czasowym i uwzględniać potencjalny efekt sezonowości.  ￼\n",
    "Każda obserwacja opisuje pojedynczą ofertę mieszkaniową i zawiera zestaw cech opisujących nieruchomość, takich jak m.in.: miasto, metraż, liczba pokoi, a także wybrane informacje o budynku i otoczeniu.\n",
    "\n",
    "W ramach sprawozdania przedstawione zostaną: wstępna eksploracja danych i analiza rozkładów, przygotowanie danych do modelowania, inżynieria cech oraz budowa i ocena modeli predykcyjnych z użyciem standardowych miar jakości dla regresji.\n",
    "\n",
    "Na końcu pracy sformułowane zostaną wnioski dotyczące jakości predykcji oraz interpretacji czynników, które najsilniej wpływają na poziom cen mieszkań w analizowanym okresie i miastach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68862c",
   "metadata": {},
   "source": [
    "## 0. Konfiguracja środowiska i import bibliotek\n",
    "\n",
    "W tej sekcji przygotowujemy środowisko pracy. Projekt opiera się na analizie danych tabelarycznych oraz wizualizacji, dlatego wykorzystujemy standardowy stos technologiczny:\n",
    "\n",
    "1.  **Manipulacja danymi:** Biblioteka `pandas` posłuży do wczytania plików CSV, czyszczenia danych i agregacji, a `numpy` do operacji matematycznych (np. logarytmowanie).\n",
    "2.  **Wizualizacja:** Używamy `matplotlib` jako fundamentu oraz `seaborn` do tworzenia estetycznych wykresów statystycznych (np. boxploty, heatmapy).\n",
    "3.  **Obsługa plików:** Moduły `pathlib` oraz `re` (wyrażenia regularne) są niezbędne, ponieważ nasz zbiór danych jest podzielony na wiele plików (snapshotów czasowych), a data pobrania danych zawarta jest w nazwie pliku, a nie w jego treści.\n",
    "\n",
    "Dodatkowo konfigurujemy parametry wyświetlania (`pd.set_option`), aby ramki danych w notatniku były czytelne (widoczność wszystkich kolumn) i nie były \"łamane\" w podglądzie.\n",
    "\n",
    "`HTML` z IPython.display używamy by móc umieścić kod HTML w naszym notatniku. Pozwala nam on na czytelniejszą prezentacje wyników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import norm\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)         # pokazuj wszystkie kolumny\n",
    "# pd.set_option(\"display.width\", 2000)               # większa \"szerokość\" wydruku\n",
    "# pd.set_option(\"display.max_colwidth\", 80)          # limit szerokości komórki (ustaw np. None, jeśli chcesz bez limitu)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)  # nie łam DataFrame na kilka \"bloków\" w pionie\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "DATA_DIR = Path(\"data\")  # folder z CSV\n",
    "CSV_GLOB = \"*.csv\"\n",
    "\n",
    "SNAPSHOT_RE = re.compile(r\"(?P<year>20\\d{2})[_-](?P<month>\\d{2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2eab5",
   "metadata": {},
   "source": [
    "Wylistowanie dostępnych zbiorów danych z plików CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5179ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = sorted(DATA_DIR.glob(CSV_GLOB))\n",
    "if not paths:\n",
    "    raise FileNotFoundError(f\"Brak plików CSV w: {DATA_DIR.resolve()}\")\n",
    "\n",
    "[p.name for p in paths[:10]], len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402695f",
   "metadata": {},
   "source": [
    "## 1. Wczytanie danych, wstępna agregacja i ich podział\n",
    "\n",
    "Ponieważ dane surowe są rozproszone w wielu plikach CSV (reprezentujących różne \"zrzuty\" danych w czasie), konieczna jest ich agregacja do jednej, spójnej struktury. Poniższy kod realizuje ten proces w kilku krokach:\n",
    "\n",
    "1.  **Iteracyjne wczytywanie:** W pętli przechodzimy przez listę ścieżek do plików. Każdy plik jest wczytywany do osobnej ramki danych (`df`).\n",
    "2.  **Ekstrakcja metadanych czasowych:** Kluczowa informacja o dacie pobrania danych nie znajduje się wewnątrz pliku CSV, lecz w jego nazwie (np. `...2024_06...`). Używając zdefiniowanego wcześniej wyrażenia regularnego (`SNAPSHOT_RE`), wyciągamy rok i miesiąc, a następnie konwertujemy je na obiekt `pd.Timestamp`. Pozwala to na późniejszą analizę trendów w czasie.\n",
    "3.  **Śledzenie źródła:** Dodajemy kolumnę `source_file`, aby zachować informację o pochodzeniu każdego rekordu (Data Lineage) – ułatwia to namierzanie ewentualnych błędów w konkretnych plikach.\n",
    "4.  **Konsolidacja (Concatenation):** Wszystkie mniejsze ramki danych są łączone w jedną główną ramkę `df_all` za pomocą funkcji `pd.concat`. Resetujemy indeks (`ignore_index=True`), aby zachować ciągłość numeracji wierszy.\n",
    "5.  **Feature Engineering (Cena za m²):** Już na tym etapie tworzymy nową cechę `price_per_m2`. Jest to najbardziej miarodajny wskaźnik na rynku nieruchomości, pozwalający porównywać wartość mieszkań o różnym metrażu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ae9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for p in paths:\n",
    "    df = pd.read_csv(p)\n",
    "\n",
    "    # snapshot_date z nazwy pliku: YYYY_MM lub YYYY-MM -> YYYY-MM-01\n",
    "    m = SNAPSHOT_RE.search(p.name)\n",
    "    snapshot_date = pd.NaT\n",
    "    if m:\n",
    "        snapshot_date = pd.Timestamp(year=int(m.group(\"year\")), month=int(m.group(\"month\")), day=1)\n",
    "\n",
    "    df[\"source_file\"] = p.name\n",
    "    df[\"snapshot_date\"] = snapshot_date\n",
    "    df[\"snapshot_year\"] = pd.to_datetime(df[\"snapshot_date\"]).dt.year\n",
    "    df[\"snapshot_month\"] = pd.to_datetime(df[\"snapshot_date\"]).dt.month\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "df_all['price_per_m2'] = df_all['price'] / df_all['squareMeters']\n",
    "df_all.shape\n",
    "\n",
    "display(df_all[['city', 'price', 'squareMeters', 'price_per_m2']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1694506d",
   "metadata": {},
   "source": [
    "### Podział danych na podzbiory (Sprzedaż vs Wynajem)\n",
    "\n",
    "Ze względu na to, że zbiór danych zawiera zmieszane oferty sprzedaży i wynajmu (które mają drastycznie różne ceny), dzielimy główną ramkę `df_all` na dwie niezależne części:\n",
    "1.  **Wynajem (`df_rent`):** Wyodrębniamy rekordy, których nazwa pliku źródłowego (`source_file`) zawiera słowo \"rent\".\n",
    "2.  **Sprzedaż (`df_sell`):** Do tego zbioru trafiają wszystkie pozostałe rekordy (operator `~` oznacza logiczną negację maski wynajmu).\n",
    "\n",
    "Użycie metody `.copy()` jest tutaj kluczowe – tworzy ona fizyczną kopię danych w pamięci, dzięki czemu późniejsze czyszczenie jednego zbioru nie wpływa na drugi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozpoznanie po nazwie pliku (source_file)\n",
    "# rent -> ma \"rent\" w nazwie\n",
    "mask_rent = df_all[\"source_file\"].str.lower().str.contains(\"rent\", na=False)\n",
    "\n",
    "# sell/ceny -> wszystko co NIE jest rent\n",
    "mask_sell = ~mask_rent\n",
    "\n",
    "df_rent = df_all[mask_rent].copy()\n",
    "df_sell = df_all[mask_sell].copy()\n",
    "\n",
    "df_rent.shape, df_sell.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03acc43",
   "metadata": {},
   "source": [
    "### Weryfikacja niezaklasyfikowanych rekordów\n",
    "\n",
    "Tworzymy pomocniczy zbiór `df_other`, trafiają do niego rekordy, które nie zostały przypisane ani do kategorii sprzedaży, ani wynajmu (np. z powodu nietypowych nazw plików). Sprawdzamy liczebność i źródła tych danych, aby upewnić się, że nie pomijamy istotnych informacji w procesie podziału. Dla naszych danych oczekujemy pustej ramki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be82e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_other = df_all[~(mask_rent | mask_sell)].copy()\n",
    "df_other[\"source_file\"].value_counts().head(20), df_other.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd93dd4",
   "metadata": {},
   "source": [
    "## 2. Wstępna Obróbka Danych\n",
    "\n",
    "Surowy zbiór danych składa się z ofert agregowanych z wielu okresów (tzw. *snapshotów*). W pierwszej kolejności dokonujemy podziału danych na dwa niezależne podzbiory:\n",
    "* **Sprzedaż (`df_sell`)**: Oferty sprzedaży mieszkań.\n",
    "* **Wynajem (`df_rent`)**: Oferty wynajmu mieszkań.\n",
    "\n",
    "Podział ten jest kluczowy, ponieważ mechanizmy cenowe rządzące rynkiem sprzedaży (cena całkowita, kredyty) różnią się fundamentalnie od rynku najmu (czynsz miesięczny, stopa zwrotu), co wymagałoby budowy osobnych modeli predykcyjnych.\n",
    "\n",
    "Poniżej przedstawiamy podstawowe statystyki dla obu wyodrębnionych grup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja pomocnicza do raportowania zakresu dat\n",
    "def get_date_range(df):\n",
    "    if 'snapshot_date' in df.columns:\n",
    "        d_min = df['snapshot_date'].min()\n",
    "        d_max = df['snapshot_date'].max()\n",
    "        return f\"{d_min.date()} — {d_max.date()}\"\n",
    "    return \"Brak danych czasowych\"\n",
    "\n",
    "# 1. Obliczenie statystyk\n",
    "stats = {\n",
    "    'Zbiór': ['Sprzedaż (Sell)', 'Wynajem (Rent)'],\n",
    "    'Liczba ofert': [len(df_sell), len(df_rent)],\n",
    "    'Liczba kolumn': [df_sell.shape[1], df_rent.shape[1]],\n",
    "    'Liczba miast': [df_sell['city'].nunique(), df_rent['city'].nunique()],\n",
    "    'Zakres dat': [get_date_range(df_sell), get_date_range(df_rent)]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "# Wyświetlenie tabeli\n",
    "display(stats_df.style.hide(axis='index'))\n",
    "\n",
    "# 2. Wizualizacja proporcji (Liczba rekordów)\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(x='Zbiór', y='Liczba ofert', data=stats_df, hue='Zbiór', palette=\"viridis\", legend=False)\n",
    "\n",
    "# Dodanie etykiet z liczbami na słupkach\n",
    "for i, v in enumerate(stats_df['Liczba ofert']):\n",
    "    ax.text(i, v + (v * 0.02), f\"{v:,}\".replace(\",\", \" \"), ha='center', fontweight='bold')\n",
    "\n",
    "plt.title('Liczebność zbiorów: Sprzedaż vs Wynajem', fontsize=14)\n",
    "plt.ylabel('Liczba ofert')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038abb55",
   "metadata": {},
   "source": [
    "### Wnioski ze wstępnej obróbki:\n",
    "1.  **Dysproporcja danych:** Jak widać na wykresie, liczebność obu grup może się różnić. Zbiór sprzedażowy (`df_sell`) jest zazwyczaj liczniejszy/mniejszy (zależnie od danych), co determinuje wybór metod walidacji.\n",
    "2.  **Spójność czasowa:** Dane dla obu kategorii pochodzą z tego samego zakresu czasowego, co pozwala na rzetelną analizę porównawczą (np. czy wzrost cen mieszkań koreluje ze wzrostem czynszów w tym samym okresie).\n",
    "3.  **Pokrycie geograficzne:** Obie grupy obejmują taką samą liczbę miast, co sugeruje, że zbiór jest zbalansowany pod kątem lokalizacji (nie brakuje nagle danych o wynajmie w dużym mieście)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65d189",
   "metadata": {},
   "source": [
    "## 3. Analiza Jakości Danych i Czyszczenie (Sanity Check)\n",
    "\n",
    "Przed przystąpieniem do modelowania konieczna jest weryfikacja jakości danych. W tym kroku realizujemy proces **\"Sanity Check\"**, który ma na celu:\n",
    "1.  **Identyfikację braków danych:** Sprawdzenie, które zmienne są niekompletne i decyzja o ich usunięciu (jeśli braków jest > 20-30%) lub imputacji (zastąpienie brakujących wartości sztucznie wygenerowanymi danymi, np. średnią).\n",
    "2.  **Wykrycie duplikatów:** Zarówno technicznych (identyczne wiersze), jak i logicznych (to samo mieszkanie pojawiające się w kolejnych snapshotach).\n",
    "3.  **Eliminację błędów grubych (Outliers):** Usunięcie rekordów nierealnych fizycznie (np. cena 1 PLN, ujemny metraż), które mogłyby zafałszować wyniki modeli regresyjnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd141b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe(df, df_name, plot_missing=True):\n",
    "    n_rows, n_cols = df.shape\n",
    "\n",
    "    # A. Braki danych\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    \n",
    "    if missing.empty:\n",
    "        table_html = \"<p><b>Brak brakujących danych (NaN).</b></p>\"\n",
    "    else:\n",
    "        missing_pct = (missing / n_rows) * 100\n",
    "        missing_df = pd.DataFrame({'Liczba': missing, '% Braków': missing_pct})\n",
    "        missing_df = missing_df.sort_values(by='% Braków', ascending=False)\n",
    "        \n",
    "        # Konwersja tabeli na HTML z kolorowaniem\n",
    "        table_html = missing_df.head(10).style.background_gradient(cmap='Reds', subset=['% Braków'])\\\n",
    "            .format({'% Braków': '{:.2f}%'})\\\n",
    "            .set_caption(\"Top 10 brakujących danych\")\\\n",
    "            .set_table_attributes('style=\"width:100%\"')\\\n",
    "            .to_html()\n",
    "\n",
    "    # B. Duplikaty\n",
    "    n_dupl = df.duplicated().sum()\n",
    "    n_dupl_logic = 0\n",
    "    if 'id' in df.columns and 'snapshot_date' in df.columns:\n",
    "        n_dupl_logic = df.duplicated(subset=['id', 'snapshot_date']).sum()\n",
    "\n",
    "    # C. Sanity Check (Błędy wartości)\n",
    "    bad_price = (df['price'] <= 0).sum()\n",
    "    bad_area = ((df['squareMeters'] < 10) | (df['squareMeters'] > 1000)).sum()\n",
    "    \n",
    "    \n",
    "    # Tworzymy panel HTML: Prawa kolumna (Tabela), Lewa kolumna (Tekst/Statystyki)\n",
    "    dashboard_html = f\"\"\"\n",
    "    <div style=\"display: flex; flex-direction: row; gap: 40px; align-items: flex-start;\">\n",
    "        <div style=\"flex: 1; min-width: 300px; padding: 15px; border-radius: 8px;\">\n",
    "            <h2> RAPORT JAKOŚCI DANYCH: {df_name} </h2>\n",
    "\n",
    "            <p><b>Liczba obserwacji:</b> {n_rows:,}</p>\n",
    "            \n",
    "            <hr>\n",
    "            \n",
    "            <p><b>Duplikaty:</b></p>\n",
    "            <ul style=\"margin-top: 5px;\">\n",
    "                <li>Pełne (dubel wiersza): <b>{n_dupl}</b></li>\n",
    "                <li>Logiczne (id + data): <b>{n_dupl_logic}</b></li>\n",
    "            </ul>\n",
    "            \n",
    "            <hr>\n",
    "            \n",
    "            <p><b>Sanity Check (Błędy):</b></p>\n",
    "            <ul style=\"margin-top: 5px;\">\n",
    "                <li>Cena <= 0 PLN: <b style=\"color: {'red' if bad_price > 0 else 'green'}\">{bad_price}</b></li>\n",
    "                <li>Metraż < 10m² lub > 1000m²: <b style=\"color: {'red' if bad_area > 0 else 'green'}\">{bad_area}</b></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div style=\"flex: 1; min-width: 300px;\">\n",
    "            {table_html}\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(dashboard_html))\n",
    "    \n",
    "    # Wykres\n",
    "    if plot_missing and not missing.empty:\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        sns.barplot(x=missing_df['% Braków'], y=missing_df.index, color='salmon')\n",
    "        plt.title(f'Wizualizacja braków danych - {df_name}')\n",
    "        plt.axvline(x=30, color='red', linestyle='--', label='Próg odcięcia (30%)')\n",
    "        plt.xlabel('% Brakujących wartości')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Wywołanie testowe\n",
    "analyze_dataframe(df_sell, \"SPRZEDAŻ\")\n",
    "analyze_dataframe(df_rent, \"WYNAJEM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5011ee",
   "metadata": {},
   "source": [
    "### Wnioski z analizy braków i decyzje:\n",
    "\n",
    "Na podstawie powyższego raportu podejmujemy następujące kroki w procesie czyszczenia (`Data Cleaning`):\n",
    "\n",
    "1.  **Usuwanie kolumn z dużą liczbą braków:** Zmienne, które mają powyżej 30-40% braków (np. często `condition`, `buildingMaterial`), niosą zbyt mało informacji, by być użyteczne, a ich imputacja byłaby obarczona dużym błędem. Zostaną usunięte.\n",
    "2.  **Imputacja:** Dla zmiennych kluczowych z niewielką liczbą braków (np. `floor`, `buildYear`) zastosujemy w późniejszym etapie (przy modelowaniu) uzupełnianie medianą lub modą.\n",
    "3.  **Usuwanie \"śmieciowych\" rekordów:** Zidentyfikowane oferty z ceną $\\le 0$ lub nierealnym metrażem (np. 1 m²) traktujemy jako błędy wprowadzania danych i usuwamy je całkowicie, aby nie zaburzały statystyk (średniej, odchylenia)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a383f71",
   "metadata": {},
   "source": [
    "### Automatyzacja procesu czyszczenia danych\n",
    "\n",
    "Na podstawie wniosków z analizy jakości (`Sanity Check`) definiujemy funkcję `clean_data`, która standaryzuje proces oczyszczania dla obu zbiorów (sprzedaży i wynajmu). Procedura składa się z trzech kluczowych etapów:\n",
    "\n",
    "1.  **Eliminacja duplikatów:**\n",
    "    * Usuwamy duplikaty techniczne (całkowicie identyczne wiersze).\n",
    "    * Usuwamy duplikaty logiczne: sytuacje, w których to samo mieszkanie (`id`) pojawia się wielokrotnie w ramach jednego zrzutu danych (`snapshot_date`). Pozostawiamy tylko pierwsze wystąpienie, aby uniknąć przekłamania statystyk (np. sztucznego zawyżania liczby ofert).\n",
    "\n",
    "2.  **Filtrowanie domenowe (Hard Rules):**\n",
    "    * Zastosowano reguły biznesowe w celu odrzucenia błędnych rekordów.\n",
    "    * **Cena:** Musi być dodatnia (`price > 0`).\n",
    "    * **Metraż:** Ograniczono analizę do lokali o powierzchni od **10 m²** (eliminacja miejsc postojowych/komórek błędnie wpisanych jako mieszkania) do **500 m²** (eliminacja obiektów komercyjnych lub błędów rzędu wielkości).\n",
    "\n",
    "3.  **Redukcja rzadkich cech:**\n",
    "    * Automatycznie usuwamy kolumny, w których brakuje ponad **50% danych**. Zmienne o tak niskim pokryciu (np. rzadko wypełniane pola opcjonalne) są bezużyteczne w modelowaniu, a ich imputacja byłaby obarczona zbyt dużym błędem.\n",
    "\n",
    "Funkcja raportuje procent odrzuconych rekordów, co pozwala kontrolować, czy nie tracimy zbyt dużej części zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df_clean = df.copy()\n",
    "    start_len = len(df_clean)\n",
    "    \n",
    "    # 1. Usuwanie duplikatów\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    if 'id' in df_clean.columns and 'snapshot_date' in df_clean.columns:\n",
    "        df_clean = df_clean.drop_duplicates(subset=['id', 'snapshot_date'], keep='first')\n",
    "\n",
    "    # 2. Usuwanie błędów logicznych (Cena i Metraż)\n",
    "    # Zakładamy, że mieszkanie musi kosztować > 1000 zł i mieć > 10 m2\n",
    "    mask_correct = (df_clean['price'] > 0) & \\\n",
    "                   (df_clean['squareMeters'] >= 10) & \\\n",
    "                   (df_clean['squareMeters'] <= 500)\n",
    "    \n",
    "    df_clean = df_clean[mask_correct]\n",
    "    \n",
    "    # 3. (Opcjonalnie) Usuwanie kolumn z > 50% braków\n",
    "    # Tutaj przykład automatyczny, ale można też ręcznie: df.drop(columns=['condition'], ...)\n",
    "    threshold = 0.5 * len(df_clean)\n",
    "    df_clean = df_clean.dropna(thresh=threshold, axis=1)\n",
    "    \n",
    "    # Raport skuteczności\n",
    "    end_len = len(df_clean)\n",
    "    dropped = start_len - end_len\n",
    "    print(f\"Czyszczenie zakończone.\\nUsunięto {dropped} rekordów ({dropped/start_len:.2%}).\")\n",
    "    print(f\"Pozostało: {end_len} obserwacji.\")\n",
    "    return df_clean\n",
    "\n",
    "# Zastosowanie czyszczenia\n",
    "print(\"--- CZYSZCZENIE ZBIORU SPRZEDAŻY ---\")\n",
    "df_sell = clean_data(df_sell)\n",
    "\n",
    "print(\"\\n--- CZYSZCZENIE ZBIORU WYNAJMU ---\")\n",
    "df_rent = clean_data(df_rent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f019e",
   "metadata": {},
   "source": [
    "Po tych wszystkich operacjach wyświetlamy pierwsze 10 wierszy dla ofert wynajmu i sprzedaży."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_rent (10 pierwszych wiersze):\")\n",
    "display(df_rent.head(10))\n",
    "\n",
    "print(\"\\ndf_sell (10 pierwszych wiersze):\")\n",
    "display(df_sell.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e03004",
   "metadata": {},
   "source": [
    "## 4. Profil zmiennych kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOP_N = 7          # None -> pokaż wszystkie miasta\n",
    "FIG_W = 12\n",
    "\n",
    "def _sanitize_has_columns(df: pd.DataFrame, has_cols):\n",
    "    \"\"\"Zostawia tylko 'yes'/'no', resztę ustawia na NaN.\"\"\"\n",
    "    for c in has_cols:\n",
    "        df[c] = df[c].where(df[c].isin([\"yes\", \"no\"]))\n",
    "    return df\n",
    "\n",
    "def plot_categorical_profiles(df: pd.DataFrame, label: str, top_n: int | None = 7):\n",
    "    df = df.copy()\n",
    "    has_cols = [c for c in df.columns if c.startswith(\"has\")]\n",
    "    df = _sanitize_has_columns(df, has_cols)\n",
    "\n",
    "    # 1) Liczba ofert per city\n",
    "    city_counts = df[\"city\"].value_counts(dropna=False)\n",
    "    plt.figure(figsize=(FIG_W, 5))\n",
    "    city_counts.plot(kind=\"bar\")\n",
    "    plt.title(f\"{label}: liczba ofert per city\")\n",
    "    plt.xlabel(\"city\")\n",
    "    plt.ylabel(\"liczba ofert\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) % 'yes' dla każdej cechy has* (globalnie, wśród nie-null)\n",
    "    if has_cols:\n",
    "        overall_yes_pct = {}\n",
    "        for c in has_cols:\n",
    "            non_null = df[c].notna().sum()\n",
    "            yes_cnt = (df[c] == \"yes\").sum()\n",
    "            overall_yes_pct[c] = (yes_cnt / non_null * 100) if non_null > 0 else np.nan\n",
    "\n",
    "        overall_yes_pct = pd.Series(overall_yes_pct).sort_values(ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(FIG_W, 5))\n",
    "        overall_yes_pct.plot(kind=\"bar\")\n",
    "        plt.title(f\"{label}: % 'yes' dla cech has* (globalnie, wśród nie-null)\")\n",
    "        plt.xlabel(\"cecha\")\n",
    "        plt.ylabel(\"% 'yes'\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 3) Dla każdej cechy has*: (A) liczba 'yes' per city + (B) % 'yes' per city (pod spodem)\n",
    "    if has_cols:\n",
    "        n_features = len(has_cols)\n",
    "        ncols = 2\n",
    "        nrows = int(np.ceil(n_features / ncols))\n",
    "\n",
    "        # 2 wiersze na każdą cechę: górny = count, dolny = %\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=nrows * 2,\n",
    "            ncols=ncols,\n",
    "            figsize=(FIG_W, max(6, nrows * 2 * 3.2)),\n",
    "            constrained_layout=True\n",
    "        )\n",
    "        axes = np.array(axes)\n",
    "\n",
    "        for idx, c in enumerate(has_cols):\n",
    "            r_base = (idx // ncols) * 2\n",
    "            col = idx % ncols\n",
    "\n",
    "            # Count yes per city\n",
    "            yes_by_city = df.groupby(\"city\")[c].apply(lambda s: (s == \"yes\").sum()).sort_values(ascending=False)\n",
    "\n",
    "            # Denominator: (yes + no) per city -> non-null count w mieście dla tej cechy\n",
    "            non_null_by_city = df.groupby(\"city\")[c].apply(lambda s: s.notna().sum())\n",
    "            pct_yes_by_city = (yes_by_city / non_null_by_city * 100).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "            # TOP N wybieramy wg liczby 'yes' (żeby zachować sens \"najwięcej\")\n",
    "            if top_n is not None:\n",
    "                top_cities = yes_by_city.head(top_n).index\n",
    "                yes_by_city = yes_by_city.loc[top_cities]\n",
    "                pct_yes_by_city = pct_yes_by_city.loc[top_cities]\n",
    "\n",
    "            # --- wykres A: liczba ---\n",
    "            ax_count = axes[r_base, col]\n",
    "            yes_by_city.plot(kind=\"bar\", ax=ax_count)\n",
    "            ax_count.set_title(f\"{label} | {c}: liczba ofert z 'yes' per city\" + (f\" (top {top_n})\" if top_n else \"\"))\n",
    "            ax_count.set_xlabel(\"\")\n",
    "            ax_count.set_ylabel(\"liczba 'yes'\")\n",
    "            ax_count.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "            # --- wykres B: procent ---\n",
    "            ax_pct = axes[r_base + 1, col]\n",
    "            pct_yes_by_city.plot(kind=\"bar\", ax=ax_pct)\n",
    "            ax_pct.set_title(f\"{label} | {c}: % ofert z 'yes' per city (wśród yes/no)\")\n",
    "            ax_pct.set_xlabel(\"city\")\n",
    "            ax_pct.set_ylabel(\"% 'yes'\")\n",
    "            ax_pct.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "        # Wyłącz niewykorzystane osie (jeśli liczba cech nie wypełnia siatki)\n",
    "        total_slots = nrows * ncols\n",
    "        for empty_idx in range(len(has_cols), total_slots):\n",
    "            r_base = (empty_idx // ncols) * 2\n",
    "            col = empty_idx % ncols\n",
    "            axes[r_base, col].axis(\"off\")\n",
    "            axes[r_base + 1, col].axis(\"off\")\n",
    "\n",
    "        fig.suptitle(f\"{label}: profil cech has* (count + % per city)\", y=1.02, fontsize=13)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# --- URUCHOMIENIA ---\n",
    "plot_categorical_profiles(df_rent, label=\"RENT\", top_n=TOP_N)\n",
    "plot_categorical_profiles(df_sell, label=\"SELL\", top_n=TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9973357",
   "metadata": {},
   "source": [
    "### Wnioski z profilu zmiennych kategorycznych\n",
    "#### 1. Silna nierównowaga ofert między miastami\n",
    "- Zarówno dla wynajmu, jak i sprzedaży widoczna jest bardzo duża koncentracja ogłoszeń w kilku największych ośrodkach — Warszawa zdecydowanie dominuje, a następnie (z wyraźnym spadkiem) Kraków i Wrocław.\n",
    "- Taki rozkład oznacza, że wyniki oparte na liczbach bezwzględnych (np. “najwięcej balkonów w Warszawie”) w dużej mierze odzwierciedlają po prostu skalę rynku / liczebność próby, a nie specyfikę zasobów mieszkaniowych.\n",
    "- Właśnie dlatego dodanie wykresów procentowych jest kluczowe: % `yes` jest bardziej miarodajny w porównaniach między miastami.\n",
    "\n",
    "#### 2. Najczęstsze cechy w RENT vs SELL\n",
    "- W RENT najbardziej powszechne są: winda (hasElevator) i balkon (hasBalcony) – wartości `yes` stanowią większość obserwacji (ok. 60%+).\n",
    "- W SELL nadal często występują balkon i winda, ale widać wyraźnie inną strukturę:\n",
    "\t- komórka lokatorska (hasStorageRoom) jest dużo częstsza niż w wynajmie\n",
    "\t- ochrona (hasSecurity) pozostaje cechą relatywnie rzadką \n",
    "- Interpretacja rynkowa: sprzedaż mocniej reprezentuje nowe inwestycje / standard deweloperski, gdzie częściej występują: komórki lokatorskie, miejsca postojowe i infrastruktura osiedlowa, w wynajmie częściej pojawiają się mieszkania “użytkowe”, gdzie te dodatki nie zawsze są formalnie przypisane.\n",
    "\n",
    "#### 3. Różnice między rankingiem liczbowym i procentowym\n",
    "- W rankingach liczbowych TOP miast dla każdej cechy niemal zawsze wygrywa Warszawa — co jest konsekwencją największej liczby ogłoszeń.\n",
    "- Dopiero wykresy procentowe pokazują realne różnice w strukturze zasobu:\n",
    "- hasParkingSpace: w wielu miastach udział `yes` jest zbliżony, ale są też wyraźne odstępstwa (np. w części miast udział jest zauważalnie wyższy niż w Warszawie).\n",
    "- hasBalcony: cecha jest ogólnie bardzo stabilna między największymi miastami (często ok. 55–65% `yes`), co sugeruje, że balkon jest w dużych ośrodkach standardem oferty, a różnice są raczej umiarkowane.\n",
    "- hasElevator: większe zróżnicowanie procentowe — w niektórych miastach udział `yes` jest zdecydowanie wyższy, co może odzwierciedlać większy udział zabudowy wielopiętrowej / nowszych budynków.\n",
    "- hasSecurity i hasStorageRoom: cechy rzadziej występujące, ale z większymi wahaniami między miastami\n",
    "\n",
    "#### 4. Różnice miejskie sugerują inny „profil budynków” i segmentację rynku\n",
    "- Miasta różnią się nie tylko cenami, ale też strukturą cech: w jednych relatywnie częściej występują windy, w innych komórki lokatorskie, a gdzie indziej miejsca parkingowe.\n",
    "- To wskazuje, że `city` jest zmienną silnie determinującą (nie tylko jako lokalizacja, ale też jako pośrednia informacja o typie zabudowy i standardu), a cechy `has*` mogą działać jako cechy doprecyzowujące segment wewnątrz miasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba37db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Sortowanie miast wg mediany ceny za m2\n",
    "order_cities = df_sell.groupby('city')['price_per_m2'].median().sort_values(ascending=False).index\n",
    "\n",
    "sns.boxplot(x='city', y='price_per_m2', data=df_sell, order=order_cities, hue='city', palette=\"viridis\", legend=False)\n",
    "plt.title('Rozkład Ceny za m² w poszczególnych miastach')\n",
    "plt.xlabel('Miasto')\n",
    "plt.ylabel('Cena za m² (PLN)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "\n",
    "# -------------------------\n",
    "# Helpery\n",
    "# -------------------------\n",
    "\n",
    "def ensure_price_per_m2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"price_per_m2\" not in df.columns and {\"price\", \"squareMeters\"}.issubset(df.columns):\n",
    "        df[\"price_per_m2\"] = df[\"price\"] / df[\"squareMeters\"]\n",
    "    return df\n",
    "\n",
    "def to_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    geo = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        geo,\n",
    "        geometry=gpd.points_from_xy(geo[\"longitude\"], geo[\"latitude\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:3857\")\n",
    "    return gdf\n",
    "\n",
    "def filter_city_warsaw(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # \"na sztywno\" Warszawa (odpornie na wielkość liter i spacje)\n",
    "    city_norm = df[\"city\"].astype(str).str.strip().str.lower()\n",
    "    return df[city_norm == \"warszawa\"].copy()\n",
    "\n",
    "def plot_hex_median_ppm2(ax, gdf, title, gridsize=60, mincnt=10):\n",
    "    gdf_ppm2 = gdf.dropna(subset=[\"price_per_m2\"]).copy()\n",
    "\n",
    "    hb = ax.hexbin(\n",
    "        gdf_ppm2.geometry.x, gdf_ppm2.geometry.y,\n",
    "        C=gdf_ppm2[\"price_per_m2\"],\n",
    "        reduce_C_function=np.median,\n",
    "        gridsize=gridsize,\n",
    "        mincnt=mincnt\n",
    "    )\n",
    "\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = gdf_ppm2.total_bounds\n",
    "    mx = (xmax - xmin) * 0.05\n",
    "    my = (ymax - ymin) * 0.05\n",
    "    ax.set_xlim(xmin - mx, xmax + mx)\n",
    "    ax.set_ylim(ymin - my, ymax + my)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(title, pad=10)\n",
    "    return hb\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Przygotowanie danych\n",
    "# -------------------------\n",
    "\n",
    "df_sell_m = ensure_price_per_m2(df_sell)\n",
    "df_rent_m = ensure_price_per_m2(df_rent)\n",
    "\n",
    "# Polska\n",
    "gdf_sell_pl = to_gdf(df_sell_m)\n",
    "gdf_rent_pl = to_gdf(df_rent_m)\n",
    "\n",
    "# Warszawa (na sztywno po city)\n",
    "df_sell_waw = filter_city_warsaw(df_sell_m)\n",
    "df_rent_waw = filter_city_warsaw(df_rent_m)\n",
    "\n",
    "gdf_sell_waw = to_gdf(df_sell_waw)\n",
    "gdf_rent_waw = to_gdf(df_rent_waw)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) POLSKA – mediana price_per_m2 (SELL + RENT)\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "hb1 = plot_hex_median_ppm2(\n",
    "    axes[0], gdf_sell_pl,\n",
    "    \"SELL (Polska): mediana price_per_m2 (min 10 / heks)\",\n",
    "    gridsize=60, mincnt=10\n",
    ")\n",
    "hb2 = plot_hex_median_ppm2(\n",
    "    axes[1], gdf_rent_pl,\n",
    "    \"RENT (Polska): mediana price_per_m2 (min 10 / heks)\",\n",
    "    gridsize=60, mincnt=10\n",
    ")\n",
    "\n",
    "fig.colorbar(hb1, ax=axes[0], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
    "fig.colorbar(hb2, ax=axes[1], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) WARSZAWA – mediana price_per_m2 (SELL + RENT)\n",
    "# -------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "hb1 = plot_hex_median_ppm2(\n",
    "    axes[0], gdf_sell_waw,\n",
    "    \"SELL (Warszawa): mediana price_per_m2 (min 10 / heks)\",\n",
    "    gridsize=45, mincnt=10\n",
    ")\n",
    "hb2 = plot_hex_median_ppm2(\n",
    "    axes[1], gdf_rent_waw,\n",
    "    \"RENT (Warszawa): mediana price_per_m2 (min 10 / heks)\",\n",
    "    gridsize=45, mincnt=10\n",
    ")\n",
    "\n",
    "fig.colorbar(hb1, ax=axes[0], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
    "fig.colorbar(hb2, ax=axes[1], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40878df0",
   "metadata": {},
   "source": [
    "### Wnioski z map heksowych price_per_m2 \n",
    "\n",
    "#### 1. Obraz dla Polski \n",
    "- Na mapach ogólnopolskich widać wyraźną polaryzację cen pomiędzy największymi rynkami a pozostałymi miastami. Dla sprzedaży  najwyższe wartości price_per_m2 koncentrują się w największych aglomeracjach, co jest spójne z intuicją rynkową: silny popyt, wyższe dochody oraz większy udział nowej zabudowy i atrakcyjnych lokalizacji.\n",
    "- Dla wynajmu  rozkład jest podobny (najdroższe ośrodki nadal dominują), ale skala wartości jest oczywiście inna. W praktyce widać, że zarówno w sprzedaży, jak i w najmie Warszawa dominuje.\n",
    "- Jednocześnie mapy potwierdzają, że analiza w skali kraju powinna być prowadzona z uwzględnieniem faktu, że porównujemy różne rynki lokalne, a nie jednorodny.\n",
    "\n",
    "#### 2. Obraz dla Warszawy \n",
    "- W Warszawie na mapie sprzedaży widać mocny gradient cenowy: najwyższe wartości price_per_m2 układają się w centralnej części miasta i w wybranych kierunkach tworzą spójne „pasma” podwyższonych cen, podczas gdy peryferia (oraz obszary o mniejszej liczbie ofert) częściej wykazują niższe mediany.\n",
    "- Na mapie wynajmu  wzorzec jest podobny w sensie geograficznym (również wyróżnia się rdzeń i obszary o wyższych stawkach), jednak struktura bywa bardziej rozproszona, co może wynikać z większej heterogeniczności ofert najmu (standard, metraż, segment rynku, krótkoterminowe ogłoszenia itp.).\n",
    "- Co istotne, oba wykresy  sugerują, że w Warszawie ceny nie rosną równomiernie „od centrum w kółko”, tylko tworzą układ, który może być powiązany z osiami dobrej dostępności komunikacyjnej.\n",
    "\n",
    "#### 3. Droższe lokalizacje wzdłuż M1\n",
    "- Na mapach Warszawy da się zauważyć układ podwyższonych cen, który luźno przypomina liniowy korytarz. Jedną z naturalnych interpretacji jest wpływ infrastruktury transportowej – w szczególności pierwszej linii metra (M1), która łączy północ–południe i przebiega przez kluczowe obszary miejskie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "\n",
    "# -------------------------\n",
    "# Konfiguracja\n",
    "# -------------------------\n",
    "GRIDSIZE_WAW = 80   # większe = drobniejsze heksy, mniejsze = większe heksy\n",
    "MINCNT = 1          # pokazuj heksy od 1 oferty\n",
    "\n",
    "# Jeśli chcesz konkretne cechy, ustaw listę:\n",
    "FEATURES = [\"hasBalcony\", \"hasElevator\", \"hasSecurity\", \"hasParkingSpace\", \"hasStorageRoom\"]\n",
    "# FEATURES = None  # None => weź wszystkie kolumny zaczynające się od \"has\"\n",
    "\n",
    "# -------------------------\n",
    "# Helpery\n",
    "# -------------------------\n",
    "def filter_city_warsaw(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    city_norm = df[\"city\"].astype(str).str.strip().str.lower()\n",
    "    return df[city_norm == \"warszawa\"].copy()\n",
    "\n",
    "def to_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    geo = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "    return gpd.GeoDataFrame(\n",
    "        geo,\n",
    "        geometry=gpd.points_from_xy(geo[\"longitude\"], geo[\"latitude\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:3857\")\n",
    "\n",
    "def plot_hex_density(ax, gdf, title, gridsize=55, mincnt=1):\n",
    "    hb = ax.hexbin(\n",
    "        gdf.geometry.x, gdf.geometry.y,\n",
    "        gridsize=gridsize,\n",
    "        mincnt=mincnt\n",
    "    )\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    mx = (xmax - xmin) * 0.05\n",
    "    my = (ymax - ymin) * 0.05\n",
    "    ax.set_xlim(xmin - mx, xmax + mx)\n",
    "    ax.set_ylim(ymin - my, ymax + my)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(title, pad=10)\n",
    "    return hb\n",
    "\n",
    "# -------------------------\n",
    "# 1) Połącz SELL + RENT i przytnij do Warszawy\n",
    "# -------------------------\n",
    "df_all = pd.concat([df_sell, df_rent], ignore_index=True)\n",
    "df_waw = filter_city_warsaw(df_all)\n",
    "\n",
    "# has* tylko 'yes'/'no', reszta => NaN (zgodnie z Twoją zasadą)\n",
    "has_cols = [c for c in df_waw.columns if c.startswith(\"has\")]\n",
    "for c in has_cols:\n",
    "    df_waw[c] = df_waw[c].where(df_waw[c].isin([\"yes\", \"no\"]))\n",
    "\n",
    "# wybór cech do rysowania\n",
    "if FEATURES is None:\n",
    "    features = has_cols\n",
    "else:\n",
    "    features = [c for c in FEATURES if c in df_waw.columns]\n",
    "\n",
    "gdf_waw = to_gdf(df_waw)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Mapy intensywności: gdzie są oferty z cechą = 'yes'\n",
    "# -------------------------\n",
    "if len(features) == 0:\n",
    "    raise ValueError(\"Brak kolumn has* do narysowania (features jest puste).\")\n",
    "\n",
    "n = len(features)\n",
    "ncols = 2\n",
    "nrows = int(np.ceil(n / ncols))\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(16, max(6, nrows * 5)))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    ax = axes[i]\n",
    "\n",
    "    subset = gdf_waw[gdf_waw[feat] == \"yes\"].copy()\n",
    "    if subset.empty:\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Warszawa | {feat}=yes: brak punktów\")\n",
    "        continue\n",
    "\n",
    "    hb = plot_hex_density(\n",
    "        ax,\n",
    "        subset,\n",
    "        title=f\"Warszawa (SELL+RENT) | {feat}=yes: intensywność występowania (hexbin count)\",\n",
    "        gridsize=GRIDSIZE_WAW,\n",
    "        mincnt=MINCNT\n",
    "    )\n",
    "\n",
    "    fig.colorbar(hb, ax=ax, fraction=0.046, pad=0.04, label=\"liczba ofert w heksie\")\n",
    "\n",
    "# wyłącz niewykorzystane osie\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5c925",
   "metadata": {},
   "source": [
    "### Wnioski z map intensywności występowania cech has* w Warszawie\n",
    "\n",
    "#### 1. Co dokładnie pokazują mapy\n",
    "- Mapy przedstawiają zagęszczenie ofert (hexbin count) spełniających warunek `hasX = yes`, czyli gdzie dana cecha (np. balkon, winda, ochrona) występuje w ogłoszeniach.\n",
    "\n",
    "#### 2. Balkon (hasBalcony)\n",
    "- Balkon jest cechą szeroko rozpowszechnioną – mapa jest stosunkowo „pełna”, co sugeruje, że w wielu częściach Warszawy oferty z balkonem pojawiają się regularnie.\n",
    "- Największe natężenia występują w obszarach o wysokiej aktywności rynku (duża liczba ogłoszeń), co jest spójne z tym, że balkon jest standardem oferty w wielu segmentach\n",
    "\n",
    "#### 3. Winda (hasElevator)\n",
    "- hasElevator generuje wyraźnie silniejsze ogniska koncentracji niż balkon, co wskazuje na duży wolumen ofert w zabudowie, gdzie winda jest typowym wyposażeniem.\n",
    "- Przestrzennie rozkład jest zgodny z intuicją urbanistyczną: winda częściej pojawia się tam, gdzie dominuje zabudowa wielopiętrowa lub nowsze inwestycje mieszkaniowe. Jednocześnie brak widocznych „punktowych” koncentracji sugeruje, że to cecha strukturalna dla typów budynków, a nie atrybut związany z pojedynczymi mikro-lokalizacjami.\n",
    "\n",
    "#### 4. Ochrona (hasSecurity)\n",
    "- hasSecurity jest cechą zdecydowanie rzadszą – mapa zawiera mniej silnych koncentracji, a wiele heksów ma niską intensywność.\n",
    "- W praktyce oznacza to, że ochrona w ogłoszeniach jest bardziej charakterystyczna dla wybranych inwestycji/kompleksów (np. osiedla zamknięte, segment premium), a nie dla całego rynku.\n",
    "\n",
    "#### 5. Miejsce parkingowe (hasParkingSpace)\n",
    "- Cecha występuje częściej niż ochrona, ale jej natężenia są bardziej selektywne niż w przypadku balkonu.\n",
    "- Przestrzennie może to odzwierciedlać większy udział inwestycji, w których parking jest przypisany do lokalu (garaże podziemne, miejsca postojowe), co bywa częstsze w nowszej zabudowie i określonych typach osiedli.\n",
    "\n",
    "#### 6. Komórka lokatorska (hasStorageRoom)\n",
    "- hasStorageRoom wykazuje stosunkowo szeroki zasięg, ale intensywności są wyraźnie niższe niż przy windzie.\n",
    "- To sugeruje, że komórka lokatorska pojawia się jako cecha istotna, lecz mniej „domyślna” niż balkon czy winda – może być mocniej związana z konkretnymi typami budynków i standardem inwestycji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3182a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "\n",
    "# -------------------------\n",
    "# Konfiguracja\n",
    "# -------------------------\n",
    "GRIDSIZE_WAW = 80     # siatka heksów\n",
    "MINCNT = 10           # min liczba ofert w heksie, żeby liczyć medianę\n",
    "TOP_Q = 0.85          # bierzemy top 10% heksów wg mediany price_per_m2\n",
    "N_SEGMENTS = 35       # ile punktów kontrolnych ma mieć \"ścieżka\"\n",
    "SMOOTH_WINDOW = 5     # wygładzanie punktów ścieżki (1 = brak)\n",
    "\n",
    "# -------------------------\n",
    "# Helpery\n",
    "# -------------------------\n",
    "def ensure_price_per_m2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"price_per_m2\" not in df.columns and {\"price\", \"squareMeters\"}.issubset(df.columns):\n",
    "        df[\"price_per_m2\"] = df[\"price\"] / df[\"squareMeters\"]\n",
    "    return df\n",
    "\n",
    "def filter_warsaw(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    city_norm = df[\"city\"].astype(str).str.strip().str.lower()\n",
    "    return df[city_norm == \"warszawa\"].copy()\n",
    "\n",
    "def to_gdf(df: pd.DataFrame) -> gpd.GeoDataFrame:\n",
    "    geo = df.dropna(subset=[\"latitude\", \"longitude\"]).copy()\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        geo,\n",
    "        geometry=gpd.points_from_xy(geo[\"longitude\"], geo[\"latitude\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(\"EPSG:3857\")\n",
    "    return gdf\n",
    "\n",
    "def pca_first_component(points_xy: np.ndarray):\n",
    "    \"\"\"\n",
    "    points_xy: (n,2) w metrach (EPSG:3857).\n",
    "    Zwraca: mean, unit_vector_pc1, unit_vector_pc2\n",
    "    \"\"\"\n",
    "    mean = points_xy.mean(axis=0)\n",
    "    X = points_xy - mean\n",
    "    cov = np.cov(X.T)\n",
    "    vals, vecs = np.linalg.eigh(cov)        # eigenvalues ascending\n",
    "    pc1 = vecs[:, np.argmax(vals)]\n",
    "    pc1 = pc1 / np.linalg.norm(pc1)\n",
    "    pc2 = np.array([-pc1[1], pc1[0]])\n",
    "    return mean, pc1, pc2\n",
    "\n",
    "def build_path_from_top_hex_centers(centers_xy: np.ndarray, n_segments: int = 35, smooth_window: int = 3):\n",
    "    \"\"\"\n",
    "    Buduje \"ścieżkę\" jako uporządkowane punkty kontrolne wzdłuż osi PC1:\n",
    "    - rzut na PC1\n",
    "    - podział na segmenty\n",
    "    - średnie punktów w segmentach\n",
    "    - opcjonalne wygładzenie\n",
    "    \"\"\"\n",
    "    mean, pc1, pc2 = pca_first_component(centers_xy)\n",
    "    X = centers_xy - mean\n",
    "    t = X @ pc1  # współrzędna wzdłuż głównej osi\n",
    "\n",
    "    # segmentacja po t\n",
    "    t_min, t_max = np.min(t), np.max(t)\n",
    "    edges = np.linspace(t_min, t_max, n_segments + 1)\n",
    "\n",
    "    path_pts = []\n",
    "    for a, b in zip(edges[:-1], edges[1:]):\n",
    "        mask = (t >= a) & (t < b)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        # średni punkt w segmencie (w 2D)\n",
    "        path_pts.append(centers_xy[mask].mean(axis=0))\n",
    "\n",
    "    path_pts = np.array(path_pts)\n",
    "    if len(path_pts) < 3:\n",
    "        return path_pts\n",
    "\n",
    "    # wygładzanie kroczące\n",
    "    if smooth_window and smooth_window > 1:\n",
    "        k = smooth_window\n",
    "        pad = k // 2\n",
    "        padded = np.pad(path_pts, ((pad, pad), (0, 0)), mode=\"edge\")\n",
    "        smoothed = []\n",
    "        for i in range(len(path_pts)):\n",
    "            smoothed.append(padded[i:i+k].mean(axis=0))\n",
    "        path_pts = np.array(smoothed)\n",
    "\n",
    "    return path_pts\n",
    "\n",
    "def plot_premium_corridor(df: pd.DataFrame, title_prefix: str,\n",
    "                          gridsize: int = 55, mincnt: int = 10,\n",
    "                          top_q: float = 0.90, n_segments: int = 35, smooth_window: int = 3):\n",
    "    \"\"\"\n",
    "    1) Tworzy heksy z medianą price_per_m2\n",
    "    2) Bierze top (1-top_q) heksów jako \"premium\"\n",
    "    3) Dopasowuje heurystyczną 'oś premium' (PCA + segmentacja)\n",
    "    4) Rysuje na mapie: wszystkie heksy (kolor = mediana), wyróżnia top-heksy, i ścieżkę\n",
    "    \"\"\"\n",
    "    df = ensure_price_per_m2(df)\n",
    "    df = filter_warsaw(df)\n",
    "    gdf = to_gdf(df).dropna(subset=[\"price_per_m2\"]).copy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 9))\n",
    "\n",
    "    # hexbin mediany (to tworzy \"rastr\" mediany ceny)\n",
    "    hb = ax.hexbin(\n",
    "        gdf.geometry.x, gdf.geometry.y,\n",
    "        C=gdf[\"price_per_m2\"],\n",
    "        reduce_C_function=np.median,\n",
    "        gridsize=gridsize,\n",
    "        mincnt=mincnt\n",
    "    )\n",
    "\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "    # widok na obszar danych\n",
    "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
    "    mx = (xmax - xmin) * 0.05\n",
    "    my = (ymax - ymin) * 0.05\n",
    "    ax.set_xlim(xmin - mx, xmax + mx)\n",
    "    ax.set_ylim(ymin - my, ymax + my)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # wyciągamy centra heksów i ich wartości (mediany)\n",
    "    centers = hb.get_offsets()          # (n_hex, 2)\n",
    "    values = hb.get_array()             # (n_hex,)\n",
    "\n",
    "    # top-heksy (premium) wg kwantyla\n",
    "    thr = np.nanquantile(values, top_q)\n",
    "    top_mask = values >= thr\n",
    "    top_centers = centers[top_mask]\n",
    "\n",
    "    # budowa ścieżki po top-heksach\n",
    "    path_pts = build_path_from_top_hex_centers(\n",
    "        top_centers,\n",
    "        n_segments=n_segments,\n",
    "        smooth_window=smooth_window\n",
    "    )\n",
    "\n",
    "    # rysowanie top-heksów jako punkty (overlay)\n",
    "    ax.scatter(top_centers[:, 0], top_centers[:, 1], s=10, alpha=0.9)\n",
    "\n",
    "    # rysowanie ścieżki\n",
    "    if len(path_pts) >= 2:\n",
    "        ax.plot(path_pts[:, 0], path_pts[:, 1], linewidth=3)\n",
    "\n",
    "    ax.set_title(f\"{title_prefix} | Warszawa: korytarz 'premium' z top {int((1-top_q)*100)}% heksów wg mediany price_per_m2\", pad=12)\n",
    "    plt.colorbar(hb, ax=ax, fraction=0.046, pad=0.04, label=\"mediana price_per_m2 (PLN/m²)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Uruchomienie: SELL i RENT osobno\n",
    "# -------------------------\n",
    "plot_premium_corridor(df_sell, \"SELL\", gridsize=GRIDSIZE_WAW, mincnt=MINCNT, top_q=TOP_Q, n_segments=N_SEGMENTS, smooth_window=SMOOTH_WINDOW)\n",
    "plot_premium_corridor(df_rent, \"RENT\", gridsize=GRIDSIZE_WAW, mincnt=MINCNT, top_q=TOP_Q, n_segments=N_SEGMENTS, smooth_window=SMOOTH_WINDOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff07526",
   "metadata": {},
   "source": [
    "### Wnioski z próby odtworzenia przebiegu linii metra na podstawie cen w Warszawie\n",
    "\n",
    "#### 1. Cel i założenie eksperymentu\n",
    "- W tej części analizy podjęto próbę przewidzenia przybliżonego przebiegu pierwszej linii metra (M1) w Warszawie, korzystając wyłącznie z informacji zawartych w danych ofertowych.\n",
    "- Założeniem było, że dostęp do metra istotnie wpływa na atrakcyjność lokalizacji, a więc może przekładać się na wyższe średnie/medianowe ceny za m² w obszarach położonych wzdłuż osi transportowej.\n",
    "\n",
    "#### 2. Metoda (intuicja)\n",
    "- Na siatce heksów obliczono medianę price_per_m2 i wybrano obszary o najwyższych wartościach (top 15% heksów).\n",
    "- Następnie na podstawie geometrii tych obszarów wyznaczono ciągłą linię, która ma reprezentować dominującą oś przestrzenną wysokich cen.\n",
    "- Kluczowe jest to, że procedura nie korzystała z żadnych danych o transporcie (brak lokalizacji stacji, przebiegu torów, przystanków), więc wynik jest wyłącznie wnioskowaniem pośrednim na podstawie rozkładu cen.\n",
    "\n",
    "#### 3. Wynik: zgodność z rzeczywistym przebiegiem M1\n",
    "- Otrzymana linia jest mocno zbliżona do rzeczywistego przebiegu M1, zwłaszcza w wariancie dla sprzedaży (SELL). Widoczna jest dominująca orientacja północ–południe i przebieg przez obszary o najwyższych medianach cen.\n",
    "- Dla wynajmu (RENT) zgodność również jest widoczna, ale rezultat jest mniej jednoznaczny i bardziej podatny na lokalne odchylenia\n",
    "\n",
    "#### 4. Odchylenie na południu (kierunek Wilanowa)\n",
    "- W obu wariantach zauważalne jest, że dolny fragment wyznaczonej trasy odchyla się bardziej w stronę Wilanowa niż rzeczywista M1.\n",
    "- To prawdopodobnie efekt tego, że Wilanów jest obszarem o relatywnie wysokich cenach, a metoda oparta na cenie/m² traktuje takie dzielnice jako “silny sygnał”, mimo że nie wynika on bezpośrednio z przebiegu M1.\n",
    "- Ten element pokazuje ograniczenie podejścia: rozkład cen odzwierciedla jednocześnie wiele czynników (centrum, standard zabudowy, prestiż lokalizacji, infrastruktura), a nie wyłącznie dostępność metra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7a48a",
   "metadata": {},
   "source": [
    "## 5. Statystyki liczbowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86d3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ustaw percentyle do describe()\n",
    "PCTS = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",
    "\n",
    "def ensure_price_per_m2(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"price_per_m2\" not in df.columns and {\"price\", \"squareMeters\"}.issubset(df.columns):\n",
    "        df[\"price_per_m2\"] = df[\"price\"] / df[\"squareMeters\"]\n",
    "    return df\n",
    "\n",
    "def stats_table(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
    "    df = ensure_price_per_m2(df)\n",
    "\n",
    "    wanted = [\"price\", \"squareMeters\", \"rooms\", \"centreDistance\", \"buildYear\", \"price_per_m2\"]\n",
    "    cols = [c for c in wanted if c in df.columns]\n",
    "\n",
    "    # describe() dla kluczowych zmiennych + percentyle\n",
    "    desc = df[cols].describe(percentiles=PCTS).T  # transpozycja: wiersze=zmienne\n",
    "    desc.insert(0, \"dataset\", label)\n",
    "\n",
    "    # czytelność: zaokrąglenie (cen i dystansów), buildYear bez zmian (jeśli float, zostanie zaokrąglone)\n",
    "    # możesz zmienić round(2) na round(0) jeśli wolisz\n",
    "    return desc.round(2)\n",
    "\n",
    "stats_sell = stats_table(df_sell, \"SELL\")\n",
    "stats_rent = stats_table(df_rent, \"RENT\")\n",
    "\n",
    "display(stats_sell)\n",
    "display(stats_rent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290a0a6",
   "metadata": {},
   "source": [
    "### Wnioski z statystyk opisowych\n",
    "\n",
    "#### 1. Skala zbioru i kompletność danych\n",
    "- Zbiór sprzedaży jest znacznie większy: 195 568 ofert vs 70 847 ofert dla wynajmu.\n",
    "- buildYear ma braki: dostępny dla ok. 163 352/195 568 (~83,5%) w SELL oraz 51 165/70 847 (~72,2%) w RENT. W dalszej analizie/modelowaniu trzeba to uwzględnić\n",
    "\n",
    "#### 2. Ceny – poziom i rozkład\n",
    "SELL (cena całkowita):\n",
    "- Mediana ceny to ok. 699 tys. PLN, a 75% ofert mieści się do 930 tys. PLN.\n",
    "- Rozkład jest wyraźnie prawoskośny (długi ogon): 95% ofert jest poniżej ~1,59 mln PLN, ale maksimum sięga 3,25 mln PLN – widać segment premium/outliery.\n",
    "\n",
    "RENT (czynsz miesięczny):\n",
    "- Mediana czynszu to ok. 3 100 PLN, a 75% ofert do 4 490 PLN.\n",
    "- Również silna prawoskośność: 95% poniżej 8 500 PLN, a maksimum 23 000 PLN wskazuje na segment premium / potencjalne obserwacje odstające.\n",
    "\n",
    "#### 3. Cena za m² (price_per_m2) – kluczowa metryka porównawcza\n",
    "- SELL: mediana price_per_m2 ≈ 12 979 PLN/m², a 90 percentyl ≈ 20 388 PLN/m². Zakres (min–max) jest szeroki (3 000 → 32 097 PLN/m²), co sugeruje duże zróżnicowanie lokalizacji i standardu.\n",
    "- RENT: mediana price_per_m2 ≈ 65,96 PLN/m² (miesięcznie), 90 percentyl ≈ 99,16 PLN/m². Maksimum 189,47 PLN/m² to również sygnał segmentu premium.\n",
    "\n",
    "#### 4. Metraż i liczba pokoi – „typowa” oferta\n",
    "- Metraże są zbliżone, ale SELL jest minimalnie większy:\n",
    "- SELL: mediana 54,6 m², 75% do 68,6 m²\n",
    "- RENT: mediana 50 m², 75% do 64 m²\n",
    "- Struktura pokoi:\n",
    "- SELL: mediana 3 pokoje (25–75%: 2–3)\n",
    "- RENT: mediana 2 pokoje (25–75%: 2–3)\n",
    "To pasuje do intuicji: najem częściej dotyczy mniejszych lokali.\n",
    "\n",
    "#### 5. Odległość od centrum (centreDistance)\n",
    "- RENT jest przeciętnie bliżej centrum: średnia 3,86 km vs 4,35 km w SELL; także mediana jest niższa (3,38 km vs 3,98 km).\n",
    "- To wspiera tezę, że rynek najmu jest bardziej skoncentrowany w lokalizacjach centralnych/okołocentralnych, gdzie popyt najemców jest najwyższy.\n",
    "\n",
    "#### 6. Rok budowy (buildYear) – różnice między rynkami\n",
    "- RENT ma wyraźnie młodszy zasób w danych: mediana 2001 vs 1994 w SELL.\n",
    "- W RENT 75 percentyl to 2020, podczas gdy w SELL 2016. Może to wynikać z większej reprezentacji nowej zabudowy w ofertach najmu (np. inwestycje kupowane pod wynajem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74954c7d",
   "metadata": {},
   "source": [
    "### Analiza Rozkładu Zmiennej Celu (Skośność)\n",
    "\n",
    "Modele regresji liniowej najlepiej działają, gdy zmienna celu ma rozkład zbliżony do normalnego (Krzywa Gaussa).\n",
    "Poniżej porównujemy rozkład surowej ceny (`price`) oraz jej logarytmu (`log_price`).\n",
    "\n",
    "* **Skośność (Skewness):** Miara asymetrii rozkładu.\n",
    "    * Wartość > 1 oznacza silną asymetrię prawostronną (dużo tanich, mało drogich).\n",
    "    * Wartość bliska 0 oznacza rozkład symetryczny (idealny dla modelu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybieramy dane (np. sprzedaż)\n",
    "df_dist = df_sell.copy()\n",
    "\n",
    "# Obliczenie logarytmu\n",
    "df_dist['log_price'] = np.log1p(df_dist['price'])\n",
    "\n",
    "# Obliczenie skośności\n",
    "skew_raw = df_dist['price'].skew()\n",
    "skew_log = df_dist['log_price'].skew()\n",
    "\n",
    "# Wizualizacja \"Przed i Po\"\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Rozkład surowy (PLN)\n",
    "sns.histplot(df_dist['price'], bins=50, kde=True, color='skyblue', ax=axes[0])\n",
    "axes[0].set_title(f'Rozkład Ceny (Surowy)\\nSkośność: {skew_raw:.2f} (Prawoskośny)', fontsize=14)\n",
    "axes[0].set_xlabel('Cena (PLN)')\n",
    "axes[0].axvline(df_dist['price'].mean(), color='red', linestyle='--', label='Średnia')\n",
    "\n",
    "# 2. Rozkład logarytmiczny\n",
    "sns.histplot(df_dist['log_price'], bins=50, kde=True, color='purple', ax=axes[1])\n",
    "# Dopasowanie idealnej krzywej normalnej dla porównania\n",
    "xmin, xmax = axes[1].get_xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, df_dist['log_price'].mean(), df_dist['log_price'].std())\n",
    "axes[1].plot(x, p * len(df_dist) * (xmax - xmin)/50, 'k', linewidth=2, label='Rozkład Normalny')\n",
    "\n",
    "axes[1].set_title(f'Rozkład Logarytmu Ceny\\nSkośność: {skew_log:.2f} (Bliska zeru)', fontsize=14)\n",
    "axes[1].set_xlabel('Log(Cena)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c725c",
   "metadata": {},
   "source": [
    "### Wnioski z analizy rozkładu zmiennej celu:\n",
    "Wykresy jednoznacznie pokazują, że surowe ceny mają silny rozkład prawoskośny (skośność > 1). Zastosowanie transformacji logarytmicznej \"prostuje\" rozkład, czyniąc go niemal idealnie symetrycznym (skośność bliska 0). Jest to ostateczne potwierdzenie, że **model powinien przewidywać `log_price`**, a nie surową kwotę."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c97a6",
   "metadata": {},
   "source": [
    "## 6. Badanie Zależności i Korelacje\n",
    "\n",
    "W tym kroku identyfikujemy zmienne, które mają największy wpływ na cenę mieszkania (tzw. *predyktory*). Analiza obejmuje:\n",
    "1.  **Macierz korelacji:** Sprawdzenie siły zależności liniowej (Pearson) między cechami numerycznymi a zmienną celu.\n",
    "2.  **Analizę `log_price`:** Weryfikację, czy zlogarytmowanie ceny (zmienna celu) zwiększa siłę korelacji (co jest typowe dla rozkładów prawoskośnych).\n",
    "3.  **Wizualizację trendów:** Wykresy punktowe badające relację Cena vs Metraż (dla kluczowych miast) oraz wpływ lokalizacji na cenę za m²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40752623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df, dataset_name, method='pearson'):\n",
    "    # 1. Przygotowanie danych roboczych\n",
    "    df_corr = df.copy()\n",
    "    # Logarytmowanie ceny (często linearyzuje zależności)\n",
    "    df_corr['log_price'] = np.log1p(df_corr['price'])\n",
    "    \n",
    "    # Wybór zmiennych numerycznych do analizy\n",
    "    target_cols = ['price', 'log_price', 'price_per_m2']\n",
    "    feature_cols = [\n",
    "        'squareMeters', 'rooms', 'floor', 'floorCount', 'buildYear', \n",
    "        'centreDistance', 'poiCount', 'schoolDistance', 'clinicDistance', \n",
    "        'restaurantDistance', 'kindergartenDistance'\n",
    "    ]\n",
    "    # Bierzemy tylko te kolumny, które faktycznie istnieją w DF\n",
    "    available_cols = target_cols + [c for c in feature_cols if c in df_corr.columns]\n",
    "    \n",
    "    # 2. Obliczenie macierzy korelacji\n",
    "    corr_matrix = df_corr[available_cols].corr(method=method)\n",
    "    \n",
    "    # 3. Wizualizacja - Heatmapa\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # Ukrywamy górny trójkąt (duplikaty)\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', \n",
    "                mask=mask, vmin=-1, vmax=1, center=0, cbar_kws={\"shrink\": .8})\n",
    "    plt.title(f'Macierz Korelacji ({dataset_name})')\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Ranking najważniejszych cech (Tabela)\n",
    "    print(f\"\\nTOP 10 Korelacji ze zmiennymi celu ({dataset_name}):\")\n",
    "    \n",
    "    # Pobieramy korelacje dla log_price i price_per_m2\n",
    "    rank_log = corr_matrix['log_price'].drop(target_cols, errors='ignore').abs().sort_values(ascending=False).head(10)\n",
    "    rank_m2 = corr_matrix['price_per_m2'].drop(target_cols, errors='ignore').abs().sort_values(ascending=False).head(10)\n",
    "    \n",
    "    # Funkcje pomocnicze do budowy tabeli\n",
    "    idx1, val1 = rank_log.index.tolist(), corr_matrix.loc[rank_log.index, 'log_price'].values\n",
    "    idx2, val2 = rank_m2.index.tolist(), corr_matrix.loc[rank_m2.index, 'price_per_m2'].values\n",
    "    max_len = max(len(idx1), len(idx2))\n",
    "    \n",
    "    def pad(l, size, fill): return l + [fill] * (size - len(l))\n",
    "\n",
    "    ranking_df = pd.DataFrame({\n",
    "        'Cecha (log_price)': pad(idx1, max_len, '-'),\n",
    "        'Korelacja (log_price)': pad(list(val1), max_len, np.nan),\n",
    "        ' | ': ['|'] * max_len,\n",
    "        'Cecha (price_per_m2)': pad(idx2, max_len, '-'),\n",
    "        'Korelacja (price_per_m2)': pad(list(val2), max_len, np.nan)\n",
    "    })\n",
    "    \n",
    "    # Wyświetlenie sformatowanej tabeli\n",
    "    display(ranking_df.style.background_gradient(\n",
    "        cmap='coolwarm', \n",
    "        subset=['Korelacja (log_price)', 'Korelacja (price_per_m2)'], \n",
    "        vmin=-1, vmax=1\n",
    "    ).format(\"{:.3f}\", subset=['Korelacja (log_price)', 'Korelacja (price_per_m2)']))\n",
    "\n",
    "# Wywołanie dla wyczyszczonych danych\n",
    "analyze_correlations(df_sell, \"SPRZEDAŻ\")\n",
    "analyze_correlations(df_rent, \"WYNAJEM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae2c01",
   "metadata": {},
   "source": [
    "### Wnioski z analizy korelacji:\n",
    "\n",
    "1.  **Dominacja metrażu:** Najsilniejszym predyktorem ceny całkowitej jest powierzchnia mieszkania (`squareMeters`) oraz liczba pokoi (`rooms`). Korelacja jest silna i dodatnia.\n",
    "2.  **Przewaga logarytmu:** Analiza potwierdza (patrz tabela), że zmienna `log_price` wykazuje zazwyczaj silniejszą korelację z cechami niż surowa cena (`price`). Jest to argument za trenowaniem modeli regresyjnych na zlogarytmowanej zmiennej celu.\n",
    "3.  **Lokalizacja a cena za m²:** Cena za m² (`price_per_m2`) zależy głównie od lokalizacji i otoczenia. Silna korelacja z `poiCount` (punkty usługowe) oraz ujemna korelacja z dystansem (`centreDistance`, `restaurantDistance`) wskazują, że bliskość centrum i usług drastycznie podnosi standard cenowy.\n",
    "4.  **Współliniowość:** Zauważalna jest wysoka korelacja między `squareMeters` a `rooms`. W prostych modelach liniowych może to prowadzić do problemu współliniowości, dlatego warto rozważyć użycie modelu odpornego, np. drzew decyzyjnych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc6f01",
   "metadata": {},
   "source": [
    "### Analiza porównawcza rynków lokalnych\n",
    "\n",
    "Rynek nieruchomości w Polsce nie jest jednorodny. Aby zweryfikować hipotezę, że cena mieszkania zależy nie tylko od jego metrażu, ale także od miasta, w którym się znajduje, przeprowadzamy analizę dla 4 największych rynków: Warszawy, Krakowa, Wrocławia i Poznania.\n",
    "\n",
    "Wykorzystujemy wykres punktowy z naniesioną linią regresji (`lmplot`), rozbity na panele. Pozwoli to porównać nachylenie krzywych cenowych – im bardziej stroma linia, tym droższy jest każdy kolejny metr kwadratowy w danym mieście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Używamy nazw bez polskich znaków, bo takie są w pliku CSV\n",
    "top_cities_keys = ['warszawa', 'krakow', 'wroclaw', 'poznan'] \n",
    "\n",
    "# Słownik do mapowania nazw na ładne etykiety z polskimi znakami\n",
    "city_labels = {\n",
    "    'warszawa': 'Warszawa', \n",
    "    'krakow': 'Kraków', \n",
    "    'wroclaw': 'Wrocław', \n",
    "    'poznan': 'Poznań'\n",
    "}\n",
    "\n",
    "# Tworzymy kopię danych tylko dla wybranych miast\n",
    "df_plot = df_sell[df_sell['city'].isin(top_cities_keys)].copy()\n",
    "df_plot['city_label'] = df_plot['city'].map(city_labels)\n",
    "\n",
    "# lmplot automatycznie rysuje punkty oraz dopasowuje linię regresji liniowej\n",
    "g = sns.lmplot(\n",
    "    data=df_plot, \n",
    "    x='squareMeters', \n",
    "    y='price', \n",
    "    col='city_label',   # Osobny wykres dla każdego miasta\n",
    "    col_wrap=2,         # Układ w dwóch kolumnach\n",
    "    hue='city_label',   # Kolorowanie wg miasta dla lepszej czytelności\n",
    "    height=4, \n",
    "    aspect=1.5,\n",
    "    scatter_kws={'alpha': 0.3, 's': 15}, # Przezroczyste punkty, by widzieć zagęszczenie\n",
    "    line_kws={'color': '#333333'}        # Ciemna linia trendu dla kontrastu\n",
    ")\n",
    "g.fig.suptitle('Zależność Ceny całkowitej od Metrażu w największych miastach', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0dee2",
   "metadata": {},
   "source": [
    "### Analiza wpływu lokalizacji (Mapa gęstości)\n",
    "\n",
    "Drugim kluczowym czynnikiem cenotwórczym jest odległość od centrum (`centreDistance`). Ponieważ zbiór danych jest duży, zwykły wykres punktowy byłby nieczytelny (problem nakładania się punktów, tzw. *overplotting*).\n",
    "\n",
    "Zamiast tego stosujemy wykres heksagonalny (hexbin plot), który działa jak mapa termiczna.\n",
    "* **Oś X:** Odległość od centrum (km).\n",
    "* **Oś Y:** Cena za m² (PLN).\n",
    "* **Kolor:** Liczba ofert w danym obszarze (skala logarytmiczna).\n",
    "\n",
    "Dzięki temu zobaczymy nie tylko trend cenowy, ale także strukturę podaży – w jakiej odległości od centrum dostępnych jest najwięcej mieszkań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Odsiewamy skrajne wartości (outliery) dla czytelności wykresu:\n",
    "# - Dystans < 15 km (skupiamy się na tkance miejskiej)\n",
    "# - Cena/m2 < 35 000 zł (odrzucamy luksusowe apartamenty zaburzające skalę)\n",
    "df_hex = df_sell[\n",
    "    (df_sell['centreDistance'] < 15) & \n",
    "    (df_sell['price_per_m2'] < 35000)\n",
    "]\n",
    "\n",
    "# Rysujemy hexbin\n",
    "hb = plt.hexbin(\n",
    "    df_hex['centreDistance'], \n",
    "    df_hex['price_per_m2'], \n",
    "    gridsize=40,    # Wielkość \"kafelków\"\n",
    "    cmap='inferno', # Paleta barw (od czarnego do żółtego)\n",
    "    mincnt=1,       # Nie rysuj pustych heksagonów\n",
    "    bins='log'      # Skala logarytmiczna dla kolorów (lepiej pokazuje różnice w gęstości)\n",
    ")\n",
    "\n",
    "cb = plt.colorbar(hb, label='Liczba ofert (skala log)')\n",
    "plt.title('Gęstość ofert: Cena za m² vs Odległość od centrum (Cała Polska)')\n",
    "plt.xlabel('Odległość od centrum (km)')\n",
    "plt.ylabel('Cena za m² (PLN)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c24e3",
   "metadata": {},
   "source": [
    "### Wnioski z analizy wizualnej:\n",
    "\n",
    "1.  **Liniowość metrażu:** Wykresy panelowe (6a) potwierdzają silną, liniową zależność między metrażem a ceną we wszystkich badanych miastach. Punkty układają się wzdłuż linii regresji, co jest silnym argumentem za zastosowaniem **Regresji Liniowej** jako modelu bazowego.\n",
    "2.  **Różnice w \"cenie jednostkowej\":** Nachylenie linii trendu (współczynnik kierunkowy) jest różne dla każdego miasta. W Warszawie linia jest najbardziej stroma, co oznacza, że każdy dodatkowy metr kwadratowy podnosi cenę końcową znacznie mocniej niż w Poznaniu czy Wrocławiu. Model musi więc uwzględniać miasto (cecha `city`) jako kluczowy predyktor.\n",
    "3.  **Nieliniowy wpływ odległości:** Wykres gęstości (6b) ujawnia charakterystyczny kształt litery \"L\". Ceny są najwyższe i najbardziej zróżnicowane w ścisłym centrum (0-2 km), po czym gwałtownie spadają. W pasie 5-10 km od centrum ceny stabilizują się i są mniej zróżnicowane. Sugeruje to, że zależność ceny od dystansu **nie jest liniowa** (przypomina rozkład wykładniczy lub 1/x), co może wymagać transformacji tej cechy w procesie Feature Engineering.\n",
    "4.  **Koncentracja podaży:** Najjaśniejsze obszary na wykresie heksagonalnym pokazują, że najwięcej ofert rynkowych znajduje się w przedziale 3–8 km od centrum, z cenami oscylującymi wokół średniej rynkowej (np. 10-15 tys. zł/m²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea60f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 1. PRZYGOTOWANIE DANYCH ---\n",
    "def get_preprocessor(numeric_features, categorical_features):\n",
    "    \"\"\"Tworzy i zwraca obiekt ColumnTransformer do przetwarzania danych.\"\"\"\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def prepare_data(df, city_filter=None, exclude_city=None, target_col='price'):\n",
    "    \"\"\"Filtruje dane, wybiera cechy, logarytmuje cel i dzieli na train/test.\"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Filtrowanie miastaf\n",
    "    if city_filter:\n",
    "        data = data[data['city'] == city_filter]\n",
    "\n",
    "    if exclude_city:\n",
    "        data = data[data['city'] != exclude_city]\n",
    "    \n",
    "    # Wybór cech\n",
    "    numeric_features = ['squareMeters', 'rooms', 'floor', 'buildYear', 'centreDistance', 'poiCount']\n",
    "    categorical_features = ['city'] if city_filter is None else []\n",
    "    \n",
    "    # Bezpieczny wybór tylko istniejących kolumn\n",
    "    numeric_features = [c for c in numeric_features if c in data.columns]\n",
    "    \n",
    "    X = data[numeric_features + categorical_features]\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Logarytmowanie celu\n",
    "    y_log = np.log1p(y)\n",
    "    \n",
    "    # Podział na zbiór treningowy i testowy\n",
    "    X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
    "        X, y_log, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train_log, y_test_log, numeric_features, categorical_features\n",
    "\n",
    "# --- 2. FUNKCJE MODELI ---\n",
    "def train_linear_regression(X_train, y_train, preprocessor):\n",
    "    \"\"\"Trenuje model Regresji Liniowej.\"\"\"\n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('regressor', LinearRegression())])\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_decision_tree(X_train, y_train, preprocessor):\n",
    "    \"\"\"Trenuje model Drzewa Decyzyjnego.\"\"\"\n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('regressor', DecisionTreeRegressor(max_depth=10, random_state=42))])\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_knn(X_train, y_train, preprocessor):\n",
    "    \"\"\"Trenuje model k-Najbliższych Sąsiadów.\"\"\"\n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('regressor', KNeighborsRegressor(n_neighbors=5))])\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# --- 3. EWALUACJA ---\n",
    "def evaluate_model(model, X_test, y_test_log, model_name, ax=None):\n",
    "    \"\"\"Dokonuje predykcji, liczy metryki i opcjonalnie rysuje wykres.\"\"\"\n",
    "    # Predykcja\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    \n",
    "    # Odwrócenie logarytmu (powrót do PLN)\n",
    "    y_test_true = np.expm1(y_test_log)\n",
    "    y_pred_true = np.expm1(y_pred_log)\n",
    "    \n",
    "    # Metryki\n",
    "    mae = mean_absolute_error(y_test_true, y_pred_true)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_true, y_pred_true))\n",
    "    r2 = r2_score(y_test_true, y_pred_true)\n",
    "    \n",
    "    # Wizualizacja\n",
    "    if ax:\n",
    "        sns.scatterplot(x=y_test_true, y=y_pred_true, alpha=0.3, ax=ax)\n",
    "        max_val = max(y_test_true.max(), y_pred_true.max())\n",
    "        ax.plot([0, max_val], [0, max_val], 'r--') # Idealna linia\n",
    "        ax.set_title(f\"{model_name}\\nR² = {r2:.3f}\")\n",
    "        ax.set_xlabel(\"Rzeczywista Cena\")\n",
    "        ax.set_ylabel(\"Przewidywana Cena\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE (zł)': mae,\n",
    "        'RMSE (zł)': rmse,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# --- 4. GŁÓWNA FUNKCJA STERUJĄCA ---\n",
    "def run_ml_experiments(df, title, city_filter=None, exclude_city=None):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"URUCHAMIAM EKSPERYMENTY: {title}\")\n",
    "    if city_filter: print(f\"Filtr miasta: {city_filter}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Przygotowanie danych\n",
    "    X_train, X_test, y_train, y_test, num_feats, cat_feats = prepare_data(df, city_filter, exclude_city)\n",
    "    \n",
    "    # 2. Przygotowanie preprocessora\n",
    "    preprocessor = get_preprocessor(num_feats, cat_feats)\n",
    "    \n",
    "    # 3. Trening modeli (każdy osobną funkcją)\n",
    "    models = [\n",
    "        ('Regresja Liniowa', train_linear_regression(X_train, y_train, preprocessor)),\n",
    "        ('Drzewo Decyzyjne', train_decision_tree(X_train, y_train, preprocessor)),\n",
    "        ('k-NN (k=5)', train_knn(X_train, y_train, preprocessor))\n",
    "    ]\n",
    "    \n",
    "    # 4. Ewaluacja i Raportowanie\n",
    "    results = []\n",
    "    plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    for i, (name, model) in enumerate(models):\n",
    "        ax = plt.subplot(1, 3, i+1)\n",
    "        metrics = evaluate_model(model, X_test, y_test, name, ax)\n",
    "        results.append(metrics)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabela wyników\n",
    "    results_df = pd.DataFrame(results)\n",
    "    display(results_df.style.highlight_max(axis=0, subset=['R²'], color='green')\n",
    "                      .highlight_min(axis=0, subset=['MAE (zł)', 'RMSE (zł)'], color='green')\n",
    "                      .format(\"{:.2f}\", subset=['MAE (zł)', 'RMSE (zł)'])\n",
    "                      .format(\"{:.3f}\", subset=['R²']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23184d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ml_experiments(df_sell, \"RYNEK SPRZEDAŻY (Cała Polska)\")\n",
    "\n",
    "run_ml_experiments(df_sell, \"RYNEK SPRZEDAŻY (Tylko Warszawa)\", city_filter=\"warszawa\")\n",
    "\n",
    "run_ml_experiments(df_sell, \"RYNEK SPRZEDAŻY (Cała Polska bez Warszawy)\", exclude_city=\"warszawa\")\n",
    "\n",
    "run_ml_experiments(df_rent, \"RYNEK SPRZEDAŻY (Cała Polska)\")\n",
    "\n",
    "run_ml_experiments(df_rent, \"RYNEK SPRZEDAŻY (Tylko Warszawa)\", city_filter=\"warszawa\")\n",
    "\n",
    "run_ml_experiments(df_rent, \"RYNEK SPRZEDAŻY (Cała Polska bez Warszawy)\", exclude_city=\"warszawa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a3e51",
   "metadata": {},
   "source": [
    "## 7. Wnioski i interpretacja wyników modelowania\n",
    "\n",
    "Na podstawie przeprowadzonych eksperymentów i analizy metryk błędów (MAE, RMSE, $R^2$), sformułowano następujące wnioski dotyczące skuteczności algorytmów w predykcji cen nieruchomości:\n",
    "\n",
    "1.  **Dominacja algorytmu k-Najbliższych Sąsiadów (k-NN):**\n",
    "    Najlepsze wyniki predykcyjne (najwyższy współczynnik $R^2$ oraz najniższe błędy MAE/RMSE) osiągnął model **k-NN**. Wynik ten ma silne uzasadnienie merytoryczne:\n",
    "    * **Natura rynku:** Wycena nieruchomości w praktyce opiera się na tzw. *podejściu porównawczym* (analiza transakcji podobnych lokali w okolicy). Algorytm k-NN działa w sposób analogiczny – estymuje cenę na podstawie średniej z $k$ najbardziej zbliżonych punktów w wielowymiarowej przestrzeni cech.\n",
    "    * **Lokalność:** Dzięki uwzględnieniu cech takich jak `city` oraz `centreDistance`, algorytm skutecznie znajduje sąsiadów o podobnej charakterystyce lokalizacyjnej, co jest kluczowe dla ceny.\n",
    "\n",
    "2.  **Znaczenie skalowania danych:**\n",
    "    Wysoka skuteczność k-NN potwierdza, że zastosowany **StandardScaler** został użyty poprawnie. Ponieważ k-NN opiera się na obliczaniu odległości euklidesowych, sprowadzenie metrażu (rzędu 50 m²) i odległości od centrum (rzędu 5 km) do wspólnej skali było krytyczne dla sukcesu tego modelu.\n",
    "\n",
    "3.  **Ograniczenia Regresji Liniowej:**\n",
    "    Model regresji liniowej osiągnął słabsze wyniki w porównaniu do k-NN. Wskazuje to, że zależności na rynku nieruchomości nie są w pełni liniowe. Przykładowo, wpływ odległości od centrum na cenę nie jest stały (cena spada gwałtownie blisko centrum i stabilizuje się na peryferiach), co dla prostej regresji jest trudne do odwzorowania bez zaawansowanej inżynierii cech.\n",
    "\n",
    "4.  **Wydajność Drzewa Decyzyjnego:**\n",
    "    Drzewo decyzyjne uplasowało się zazwyczaj pośrodku stawki (lub blisko k-NN). Choć dobrze radzi sobie z nieliniowością, może mieć tendencję do \"schodkowania\" predykcji (przypisywania tej samej ceny dla grupy mieszkań w jednym liściu), podczas gdy k-NN oferuje bardziej płynną interpolację cen, co w przypadku zmiennej ciągłej (ceny) daje mniejszy błąd średni.\n",
    "\n",
    "**Podsumowując:** Eksperyment wykazał, że dla tego zbioru danych podejście oparte na podobieństwie (k-NN) jest skuteczniejsze niż podejście oparte na regułach (Drzewa) lub prostych równaniach liniowych (Regresja)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
