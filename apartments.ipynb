{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d8fc110",
      "metadata": {},
      "source": [
        "## Wstęp\n",
        "\n",
        "Rynek mieszkaniowy w Polsce charakteryzuje się silnym zróżnicowaniem przestrzennym – ceny (oraz stawki najmu) w dużych miastach i w ich centrach potrafią znacząco odbiegać od wartości obserwowanych na obrzeżach czy w mniejszych ośrodkach. Celem niniejszego notatnika jest kompleksowa analiza ofert mieszkaniowych oraz budowa i ocena modeli uczenia maszynowego, które przewidują poziom cen na podstawie cech nieruchomości i jej otoczenia.\n",
        "\n",
        "W analizie wykorzystano publiczny zbiór danych “Apartment Prices in Poland” dostępny na platformie Kaggle (https://www.kaggle.com/datasets/krzysztofjamroz/apartment-prices-in-poland/data). Dane mają postać miesięcznych migawek ogłoszeń (pliki apartments_pl_YYYY_MM.csv dla sprzedaży oraz apartments_rent_pl_YYYY_MM.csv dla najmu), co pozwala łączyć rekordy z wielu okresów i zachować informację o dacie pobrania (snapshot_date). W notatniku dane zostały wczytane, zunifikowane i podzielone na dwa spójne podzbiory: sprzedaż (SELL) oraz najem (RENT).\n",
        "\n",
        "### Cel analizy i modelowania\n",
        "\n",
        "Notatnik realizuje dwa główne cele:\n",
        "\t1.\tEksploracyjna analiza danych (EDA) i opis rynku – ocena jakości danych, braków, rozkładów kluczowych zmiennych, różnic między miastami oraz wizualizacje przestrzenne (mapy heksowe median price_per_m2 i mapy intensywności występowania cech has* w Warszawie). Część przestrzenna obejmuje również eksperyment eksploracyjny polegający na próbie odtworzenia dominującej osi wysokich cen w Warszawie (bez użycia danych transportowych), co stanowi punkt wyjścia do dalszej weryfikacji hipotez.\n",
        "\t2.\tBudowa i porównanie modeli predykcyjnych – przygotowanie pipeline’u przetwarzania danych (imputacja braków, standaryzacja cech numerycznych, kodowanie zmiennych kategorycznych), zastosowanie transformacji log1p(price) dla stabilizacji rozkładu zmiennej celu oraz porównanie kilku modeli regresyjnych (m.in. regresja liniowa, drzewo decyzyjne, k-NN). Skuteczność oceniana jest metrykami MAE, RMSE i R², a eksperymenty wykonywane są zarówno dla całej Polski, jak i wariantów lokalnych (np. sama Warszawa vs Polska bez Warszawy).\n",
        "\n",
        "W efekcie notatnik dostarcza zarówno wniosków opisowych (jak różni się rynek między miastami i segmentami), jak i wyników ilościowych (jak dobrze wybrane modele potrafią przewidywać ceny na podstawie dostępnych cech), tworząc spójną podstawę do dalszych prac analitycznych i modelowych."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d68862c",
      "metadata": {},
      "source": [
        "## 0. Konfiguracja środowiska i import bibliotek\n",
        "\n",
        "W tej sekcji przygotowujemy środowisko pracy. Projekt opiera się na analizie danych tabelarycznych oraz wizualizacji, dlatego wykorzystujemy standardowy stos technologiczny:\n",
        "\n",
        "1.  **Manipulacja danymi:** Biblioteka `pandas` posłuży do wczytania plików CSV, czyszczenia danych i agregacji, a `numpy` do operacji matematycznych (np. logarytmowanie).\n",
        "2.  **Wizualizacja:** Używamy `matplotlib` jako fundamentu oraz `seaborn` do tworzenia estetycznych wykresów statystycznych (np. boxploty, heatmapy).\n",
        "3.  **Obsługa plików:** Moduły `pathlib` oraz `re` (wyrażenia regularne) są niezbędne, ponieważ nasz zbiór danych jest podzielony na wiele plików (snapshotów czasowych), a data pobrania danych zawarta jest w nazwie pliku, a nie w jego treści.\n",
        "\n",
        "Dodatkowo konfigurujemy parametry wyświetlania (`pd.set_option`), aby ramki danych w notatniku były czytelne (widoczność wszystkich kolumn) i nie były \"łamane\" w podglądzie.\n",
        "\n",
        "`HTML` z IPython.display używamy by móc umieścić kod HTML w naszym notatniku. Pozwala nam on na czytelniejszą prezentacje wyników."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4492294a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "from scipy.stats import norm\n",
        "import  geopandas as gpd\n",
        "import contextily as ctx\n",
        "\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.expand_frame_repr\", False)  \n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "DATA_DIR = Path(\"data\")  # folder z CSV\n",
        "CSV_GLOB = \"*.csv\"\n",
        "\n",
        "SNAPSHOT_RE = re.compile(r\"(?P<year>20\\d{2})[_-](?P<month>\\d{2})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3e2eab5",
      "metadata": {},
      "source": [
        "Wylistowanie dostępnych zbiorów danych z plików CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5179ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "paths = sorted(DATA_DIR.glob(CSV_GLOB))\n",
        "if not paths:\n",
        "    raise FileNotFoundError(f\"Brak plików CSV w: {DATA_DIR.resolve()}\")\n",
        "\n",
        "[p.name for p in paths[:10]], len(paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c402695f",
      "metadata": {},
      "source": [
        "## 1. Wczytanie danych, wstępna agregacja i ich podział\n",
        "\n",
        "Ponieważ dane surowe są rozproszone w wielu plikach CSV (reprezentujących różne \"zrzuty\" danych w czasie), konieczna jest ich agregacja do jednej, spójnej struktury. Poniższy kod realizuje ten proces w kilku krokach:\n",
        "\n",
        "1.  **Iteracyjne wczytywanie:** W pętli przechodzimy przez listę ścieżek do plików. Każdy plik jest wczytywany do osobnej ramki danych (`df`).\n",
        "2.  **Ekstrakcja metadanych czasowych:** Kluczowa informacja o dacie pobrania danych nie znajduje się wewnątrz pliku CSV, lecz w jego nazwie (np. `...2024_06...`). Używając zdefiniowanego wcześniej wyrażenia regularnego (`SNAPSHOT_RE`), wyciągamy rok i miesiąc, a następnie konwertujemy je na obiekt `pd.Timestamp`. Pozwala to na późniejszą analizę trendów w czasie.\n",
        "3.  **Śledzenie źródła:** Dodajemy kolumnę `source_file`, aby zachować informację o pochodzeniu każdego rekordu – ułatwia to namierzanie ewentualnych błędów w konkretnych plikach.\n",
        "4.  **Konsolidacja (Concatenation):** Wszystkie mniejsze ramki danych są łączone w jedną główną ramkę `df_all` za pomocą funkcji `pd.concat`. Resetujemy indeks (`ignore_index=True`), aby zachować ciągłość numeracji wierszy.\n",
        "5.  **Feature Engineering (Cena za m²):** Już na tym etapie tworzymy nową cechę `price_per_m2`. Jest to najbardziej miarodajny wskaźnik na rynku nieruchomości, pozwalający porównywać wartość mieszkań o różnym metrażu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229ae9d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = []\n",
        "\n",
        "for p in paths:\n",
        "    df = pd.read_csv(p)\n",
        "\n",
        "    m = SNAPSHOT_RE.search(p.name)\n",
        "    snapshot_date = pd.NaT\n",
        "    if m:\n",
        "        snapshot_date = pd.Timestamp(year=int(m.group(\"year\")), month=int(m.group(\"month\")), day=1)\n",
        "\n",
        "    df[\"source_file\"] = p.name\n",
        "    df[\"snapshot_date\"] = snapshot_date\n",
        "    df[\"snapshot_year\"] = pd.to_datetime(df[\"snapshot_date\"]).dt.year\n",
        "    df[\"snapshot_month\"] = pd.to_datetime(df[\"snapshot_date\"]).dt.month\n",
        "\n",
        "    dfs.append(df)\n",
        "\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "df_all['price_per_m2'] = df_all['price'] / df_all['squareMeters']\n",
        "df_all.shape\n",
        "\n",
        "display(df_all[['city', 'price', 'squareMeters', 'price_per_m2']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1694506d",
      "metadata": {},
      "source": [
        "### Podział danych na podzbiory (Sprzedaż vs Wynajem)\n",
        "\n",
        "Ze względu na to, że zbiór danych zawiera zmieszane oferty sprzedaży i wynajmu (które mają drastycznie różne ceny), dzielimy główną ramkę `df_all` na dwie niezależne części:\n",
        "1.  **Wynajem (`df_rent`):** Wyodrębniamy rekordy, których nazwa pliku źródłowego (`source_file`) zawiera słowo \"rent\".\n",
        "2.  **Sprzedaż (`df_sell`):** Do tego zbioru trafiają wszystkie pozostałe rekordy (operator `~` oznacza logiczną negację maski wynajmu).\n",
        "\n",
        "Użycie metody `.copy()` jest tutaj kluczowe – tworzy ona fizyczną kopię danych w pamięci, dzięki czemu późniejsze czyszczenie jednego zbioru nie wpływa na drugi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f34d067",
      "metadata": {},
      "outputs": [],
      "source": [
        "mask_rent = df_all[\"source_file\"].str.lower().str.contains(\"rent\", na=False)\n",
        "\n",
        "mask_sell = ~mask_rent\n",
        "\n",
        "df_rent = df_all[mask_rent].copy()\n",
        "df_sell = df_all[mask_sell].copy()\n",
        "\n",
        "df_rent.shape, df_sell.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03acc43",
      "metadata": {},
      "source": [
        "### Weryfikacja niezaklasyfikowanych rekordów\n",
        "\n",
        "Tworzymy pomocniczy zbiór `df_other`, trafiają do niego rekordy, które nie zostały przypisane ani do kategorii sprzedaży, ani wynajmu (np. z powodu nietypowych nazw plików). Sprawdzamy liczebność i źródła tych danych, aby upewnić się, że nie pomijamy istotnych informacji w procesie podziału. Dla naszych danych oczekujemy pustej ramki."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be82e20",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_other = df_all[~(mask_rent | mask_sell)].copy()\n",
        "df_other[\"source_file\"].value_counts().head(20), df_other.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd93dd4",
      "metadata": {},
      "source": [
        "## 2. Wstępna Obróbka Danych\n",
        "\n",
        "Naszą obróbke danych operamy na wcześniej wyodrębionnych ramkach: `df_sell` i `df_rent`.\n",
        "\n",
        "Podział ten jest kluczowy, ponieważ mechanizmy cenowe rządzące rynkiem sprzedaży (cena całkowita, kredyty) różnią się fundamentalnie od rynku najmu (czynsz miesięczny, stopa zwrotu).\n",
        "\n",
        "Poniżej przedstawiamy podstawowe statystyki dla obu wyodrębnionych grup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7377d0f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_date_range(df):\n",
        "    if 'snapshot_date' in df.columns:\n",
        "        d_min = df['snapshot_date'].min()\n",
        "        d_max = df['snapshot_date'].max()\n",
        "        return f\"{d_min.date()} — {d_max.date()}\"\n",
        "    return \"Brak danych czasowych\"\n",
        "\n",
        "stats = {\n",
        "    'Zbiór': ['Sprzedaż (Sell)', 'Wynajem (Rent)'],\n",
        "    'Liczba ofert': [len(df_sell), len(df_rent)],\n",
        "    'Liczba kolumn': [df_sell.shape[1], df_rent.shape[1]],\n",
        "    'Liczba miast': [df_sell['city'].nunique(), df_rent['city'].nunique()],\n",
        "    'Zakres dat': [get_date_range(df_sell), get_date_range(df_rent)]\n",
        "}\n",
        "\n",
        "stats_df = pd.DataFrame(stats)\n",
        "\n",
        "display(stats_df.style.hide(axis='index'))\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "ax = sns.barplot(x='Zbiór', y='Liczba ofert', data=stats_df, hue='Zbiór', palette=\"viridis\", legend=False)\n",
        "\n",
        "for i, v in enumerate(stats_df['Liczba ofert']):\n",
        "    ax.text(i, v + (v * 0.02), f\"{v:,}\".replace(\",\", \" \"), ha='center', fontweight='bold')\n",
        "\n",
        "plt.title('Liczebność zbiorów: Sprzedaż vs Wynajem', fontsize=14)\n",
        "plt.ylabel('Liczba ofert')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "038abb55",
      "metadata": {},
      "source": [
        "### Wnioski ze wstępnej obróbki:\n",
        "1.  **Dysproporcja danych:** Jak widać na wykresie, liczebność obu grup może się różnić. Zbiór sprzedażowy (`df_sell`) jest zazwyczaj liczniejszy/mniejszy (zależnie od danych), co determinuje wybór metod walidacji.\n",
        "2.  **Spójność czasowa:** Dane dla obu kategorii pochodzą z tego samego zakresu czasowego, co pozwala na rzetelną analizę porównawczą danych w czasie.\n",
        "3.  **Pokrycie geograficzne:** Obie grupy obejmują taką samą liczbę miast, co sugeruje, że zbiór jest zbalansowany pod kątem lokalizacji (nie brakuje nagle danych o wynajmie w dużym mieście)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba65d189",
      "metadata": {},
      "source": [
        "## 3. Analiza Jakości Danych i Czyszczenie\n",
        "\n",
        "Przed przystąpieniem do modelowania konieczna jest weryfikacja jakości danych. W tym kroku realizujemy proces czyszczenia danych, który ma na celu:\n",
        "1.  **Identyfikację braków danych:** Sprawdzenie, które zmienne są niekompletne i decyzja o ich usunięciu (jeśli braków jest > 30%) lub imputacji (zastąpienie brakujących wartości sztucznie wygenerowanymi danymi, np. średnią).\n",
        "2.  **Wykrycie duplikatów:** Zarówno technicznych (identyczne wiersze), jak i logicznych (to samo mieszkanie pojawiające się w kolejnych snapshotach).\n",
        "3.  **Eliminację błędów grubych (Outliers):** Usunięcie rekordów nierealnych fizycznie (np. cena 1 PLN, ujemny metraż, zbyt duży metraż), które mogłyby zafałszować wyniki modeli regresyjnych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd141b15",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_dataframe(df, df_name, plot_missing=True):\n",
        "    n_rows, n_cols = df.shape\n",
        "\n",
        "    # Braki danych\n",
        "    missing = df.isnull().sum()\n",
        "    missing = missing[missing > 0]\n",
        "    \n",
        "    if missing.empty:\n",
        "        table_html = \"<p><b>Brak brakujących danych (NaN).</b></p>\"\n",
        "    else:\n",
        "        missing_pct = (missing / n_rows) * 100\n",
        "        missing_df = pd.DataFrame({'Liczba': missing, '% Braków': missing_pct})\n",
        "        missing_df = missing_df.sort_values(by='% Braków', ascending=False)\n",
        "        \n",
        "        # Konwersja tabeli na HTML z kolorowaniem\n",
        "        table_html = missing_df.head(10).style.background_gradient(cmap='Reds', subset=['% Braków'])\\\n",
        "            .format({'% Braków': '{:.2f}%'})\\\n",
        "            .set_caption(\"Top 10 brakujących danych\")\\\n",
        "            .set_table_attributes('style=\"width:100%\"')\\\n",
        "            .to_html()\n",
        "\n",
        "    # Duplikaty\n",
        "    n_dupl = df.duplicated().sum()\n",
        "    n_dupl_logic = 0\n",
        "    if 'id' in df.columns and 'snapshot_date' in df.columns:\n",
        "        n_dupl_logic = df.duplicated(subset=['id', 'snapshot_date']).sum()\n",
        "\n",
        "    # Eleminacja błędów grubych\n",
        "    bad_price = (df['price'] <= 0).sum()\n",
        "    bad_area = ((df['squareMeters'] < 10) | (df['squareMeters'] > 1000)).sum()\n",
        "    \n",
        "    \n",
        "    # Panel HTML: Prawa kolumna (Tabela), Lewa kolumna (Tekst/Statystyki)\n",
        "    dashboard_html = f\"\"\"\n",
        "    <div style=\"display: flex; flex-direction: row; gap: 40px; align-items: flex-start;\">\n",
        "        <div style=\"flex: 1; min-width: 300px; padding: 15px; border-radius: 8px;\">\n",
        "            <h2> RAPORT JAKOŚCI DANYCH: {df_name} </h2>\n",
        "\n",
        "            <p><b>Liczba obserwacji:</b> {n_rows:,}</p>\n",
        "            \n",
        "            <hr>\n",
        "            \n",
        "            <p><b>Duplikaty:</b></p>\n",
        "            <ul style=\"margin-top: 5px;\">\n",
        "                <li>Pełne (dubel wiersza): <b>{n_dupl}</b></li>\n",
        "                <li>Logiczne (id + data): <b>{n_dupl_logic}</b></li>\n",
        "            </ul>\n",
        "            \n",
        "            <hr>\n",
        "            \n",
        "            <p><b>Sanity Check (Błędy):</b></p>\n",
        "            <ul style=\"margin-top: 5px;\">\n",
        "                <li>Cena <= 0 PLN: <b style=\"color: {'red' if bad_price > 0 else 'green'}\">{bad_price}</b></li>\n",
        "                <li>Metraż < 10m² lub > 1000m²: <b style=\"color: {'red' if bad_area > 0 else 'green'}\">{bad_area}</b></li>\n",
        "            </ul>\n",
        "        </div>\n",
        "        <div style=\"flex: 1; min-width: 300px;\">\n",
        "            {table_html}\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    display(HTML(dashboard_html))\n",
        "    \n",
        "    # Wykres\n",
        "    if plot_missing and not missing.empty:\n",
        "        plt.figure(figsize=(10, 3))\n",
        "        sns.barplot(x=missing_df['% Braków'], y=missing_df.index, color='salmon')\n",
        "        plt.title(f'Wizualizacja braków danych - {df_name}')\n",
        "        plt.axvline(x=30, color='red', linestyle='--', label='Próg odcięcia (30%)')\n",
        "        plt.xlabel('% Brakujących wartości')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "analyze_dataframe(df_sell, \"SPRZEDAŻ\")\n",
        "analyze_dataframe(df_rent, \"WYNAJEM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5011ee",
      "metadata": {},
      "source": [
        "### Wnioski z analizy braków i decyzje:\n",
        "\n",
        "Na podstawie powyższego raportu podejmujemy następujące kroki w procesie czyszczenia:\n",
        "\n",
        "1.  **Usuwanie kolumn z dużą liczbą braków:** Zmienne, które mają powyżej 30% braków (np. często `condition`, `buildingMaterial`), niosą zbyt mało informacji, by być użyteczne, a ich imputacja byłaby obarczona dużym błędem. Zostaną usunięte.\n",
        "2.  **Imputacja:** Dla zmiennych kluczowych z niewielką liczbą braków (np. `floor`, `buildYear`) zastosujemy w późniejszym etapie (przy modelowaniu) uzupełnianie medianą lub modą.\n",
        "3.  **Usuwanie \"śmieciowych\" rekordów:** Zidentyfikowane oferty z ceną $\\le 0$ lub nierealnym metrażem (np. 1 m²) traktujemy jako błędy wprowadzania danych i usuwamy je całkowicie, aby nie zaburzały statystyk (średniej, odchylenia)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a383f71",
      "metadata": {},
      "source": [
        "### Automatyzacja procesu czyszczenia danych\n",
        "\n",
        "Na podstawie wniosków z analizy jakości definiujemy funkcję `clean_data`, która standaryzuje proces oczyszczania dla obu zbiorów (sprzedaży i wynajmu). Procedura składa się z trzech kluczowych etapów:\n",
        "\n",
        "1.  **Eliminacja duplikatów:**\n",
        "    * Usuwamy duplikaty techniczne (całkowicie identyczne wiersze).\n",
        "    * Usuwamy duplikaty logiczne: sytuacje, w których to samo mieszkanie (`id`) pojawia się wielokrotnie w ramach jednego zrzutu danych (`snapshot_date`). Pozostawiamy tylko pierwsze wystąpienie, aby uniknąć przekłamania statystyk (np. sztucznego zawyżania liczby ofert).\n",
        "\n",
        "2.  **Filtrowanie po sztywno zdefiniowanych regułach:**\n",
        "    * Zastosowano reguły biznesowe w celu odrzucenia błędnych rekordów.\n",
        "    * **Cena:** Musi być dodatnia (`price > 0`).\n",
        "    * **Metraż:** Ograniczono analizę do lokali o powierzchni od **10 m²** (eliminacja miejsc postojowych/komórek błędnie wpisanych jako mieszkania) do **500 m²** (eliminacja obiektów komercyjnych lub błędów rzędu wielkości).\n",
        "\n",
        "3.  **Redukcja rzadkich cech:**\n",
        "    * Automatycznie usuwamy kolumny, w których brakuje ponad **50% danych**. Zmienne o tak niskim pokryciu (np. rzadko wypełniane pola opcjonalne) są bezużyteczne w modelowaniu, a ich imputacja byłaby obarczona zbyt dużym błędem.\n",
        "\n",
        "Funkcja raportuje procent odrzuconych rekordów, co pozwala kontrolować, czy nie tracimy zbyt dużej części zbioru danych."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c9ce7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_data(df):\n",
        "    df_clean = df.copy()\n",
        "    start_len = len(df_clean)\n",
        "    \n",
        "    # Usuwanie duplikatów\n",
        "    df_clean = df_clean.drop_duplicates()\n",
        "    if 'id' in df_clean.columns and 'snapshot_date' in df_clean.columns:\n",
        "        df_clean = df_clean.drop_duplicates(subset=['id', 'snapshot_date'], keep='first')\n",
        "\n",
        "    # Usuwanie błędów logicznych (Cena i Metraż)\n",
        "    mask_correct = (df_clean['price'] > 0) & \\\n",
        "                   (df_clean['squareMeters'] >= 10) & \\\n",
        "                   (df_clean['squareMeters'] <= 500)\n",
        "    \n",
        "    df_clean = df_clean[mask_correct]\n",
        "    \n",
        "    # Usuwanie kolumn z > 50% braków\n",
        "    threshold = 0.5 * len(df_clean)\n",
        "    df_clean = df_clean.dropna(thresh=threshold, axis=1)\n",
        "    \n",
        "    # Raport skuteczności\n",
        "    end_len = len(df_clean)\n",
        "    dropped = start_len - end_len\n",
        "    print(f\"Czyszczenie zakończone.\\nUsunięto {dropped} rekordów ({dropped/start_len:.2%}).\")\n",
        "    print(f\"Pozostało: {end_len} obserwacji.\")\n",
        "    return df_clean\n",
        "\n",
        "# Zastosowanie czyszczenia\n",
        "print(\"--- CZYSZCZENIE ZBIORU SPRZEDAŻY ---\")\n",
        "df_sell = clean_data(df_sell)\n",
        "\n",
        "print(\"\\n--- CZYSZCZENIE ZBIORU WYNAJMU ---\")\n",
        "df_rent = clean_data(df_rent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398f019e",
      "metadata": {},
      "source": [
        "Po tych wszystkich operacjach wyświetlamy pierwsze 10 wierszy dla ofert wynajmu i sprzedaży."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0544c395",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"df_rent (10 pierwszych wiersze):\")\n",
        "display(df_rent.head(10))\n",
        "\n",
        "print(\"\\ndf_sell (10 pierwszych wiersze):\")\n",
        "display(df_sell.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e03004",
      "metadata": {},
      "source": [
        "## 4. Profil zmiennych kategorycznych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ffe13d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "TOP_N = 7\n",
        "FIG_W = 12\n",
        "\n",
        "def plot_categorical_profiles(df: pd.DataFrame, label: str, top_n: int | None = 7):\n",
        "    df = df.copy()\n",
        "    has_cols = [c for c in df.columns if c.startswith(\"has\")]\n",
        "    for c in has_cols:\n",
        "        df[c] = df[c].where(df[c].isin([\"yes\", \"no\"]))\n",
        "\n",
        "    df[\"city\"].value_counts(dropna=False).plot(kind=\"bar\", figsize=(FIG_W, 5), title=f\"{label}: liczba ofert per city\")\n",
        "    plt.xlabel(\"city\"); plt.ylabel(\"liczba ofert\"); plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "    overall = pd.Series({\n",
        "        c: ((df[c] == \"yes\").sum() / df[c].notna().sum() * 100) if df[c].notna().sum() else np.nan\n",
        "        for c in has_cols\n",
        "    }).sort_values(ascending=False)\n",
        "    overall.plot(kind=\"bar\", figsize=(FIG_W, 5), title=f\"{label}: % 'yes' dla cech has* (globalnie, wśród nie-null)\")\n",
        "    plt.xlabel(\"cecha\"); plt.ylabel(\"% 'yes'\"); plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "    ncols = 2\n",
        "    nrows = int(np.ceil(len(has_cols) / ncols))\n",
        "    fig, axes = plt.subplots(nrows * 2, ncols, figsize=(FIG_W, max(6, nrows * 2 * 3.2)), constrained_layout=True)\n",
        "    axes = np.array(axes)\n",
        "\n",
        "    for idx, c in enumerate(has_cols):\n",
        "        r = (idx // ncols) * 2\n",
        "        col = idx % ncols\n",
        "\n",
        "        yes = df.groupby(\"city\")[c].apply(lambda s: (s == \"yes\").sum()).sort_values(ascending=False)\n",
        "        nn = df.groupby(\"city\")[c].apply(lambda s: s.notna().sum())\n",
        "        pct = (yes / nn * 100).replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        if top_n is not None:\n",
        "            top = yes.head(top_n).index\n",
        "            yes, pct = yes.loc[top], pct.loc[top]\n",
        "\n",
        "        yes.plot(kind=\"bar\", ax=axes[r, col])\n",
        "        axes[r, col].set_title(f\"{label} | {c}: liczba ofert z 'yes' per city\" + (f\" (top {top_n})\" if top_n else \"\"))\n",
        "        axes[r, col].set_xlabel(\"\"); axes[r, col].set_ylabel(\"liczba 'yes'\"); axes[r, col].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "        pct.plot(kind=\"bar\", ax=axes[r + 1, col])\n",
        "        axes[r + 1, col].set_title(f\"{label} | {c}: % ofert z 'yes' per city (wśród yes/no)\")\n",
        "        axes[r + 1, col].set_xlabel(\"city\"); axes[r + 1, col].set_ylabel(\"% 'yes'\"); axes[r + 1, col].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "    for empty_idx in range(len(has_cols), nrows * ncols):\n",
        "        r = (empty_idx // ncols) * 2\n",
        "        col = empty_idx % ncols\n",
        "        axes[r, col].axis(\"off\")\n",
        "        axes[r + 1, col].axis(\"off\")\n",
        "\n",
        "    fig.suptitle(f\"{label}: profil cech has* (count + % per city)\", y=1.02, fontsize=13)\n",
        "    plt.show()\n",
        "\n",
        "plot_categorical_profiles(df_rent, \"RENT\", TOP_N)\n",
        "plot_categorical_profiles(df_sell, \"SELL\", TOP_N)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9973357",
      "metadata": {},
      "source": [
        "### Wnioski z profilu zmiennych kategorycznych\n",
        "#### 1. Silna nierównowaga ofert między miastami\n",
        "- Zarówno dla wynajmu, jak i sprzedaży widoczna jest bardzo duża koncentracja ogłoszeń w kilku największych ośrodkach — Warszawa zdecydowanie dominuje, a następnie (z wyraźnym spadkiem) Kraków i Wrocław.\n",
        "- Taki rozkład oznacza, że wyniki oparte na liczbach bezwzględnych (np. “najwięcej balkonów w Warszawie”) w dużej mierze odzwierciedlają po prostu skalę rynku / liczebność próby, a nie specyfikę zasobów mieszkaniowych.\n",
        "- Właśnie dlatego dodanie wykresów procentowych jest kluczowe: % `yes` jest bardziej miarodajny w porównaniach między miastami.\n",
        "\n",
        "#### 2. Najczęstsze cechy w RENT vs SELL\n",
        "- W RENT najbardziej powszechne są: winda (hasElevator) i balkon (hasBalcony) – wartości `yes` stanowią większość obserwacji (ok. 60%+).\n",
        "- W SELL nadal często występują balkon i winda, ale widać wyraźnie inną strukturę:\n",
        "\t- komórka lokatorska (hasStorageRoom) jest dużo częstsza niż w wynajmie\n",
        "\t- ochrona (hasSecurity) pozostaje cechą relatywnie rzadką \n",
        "- Interpretacja rynkowa: sprzedaż mocniej reprezentuje nowe inwestycje / standard deweloperski, gdzie częściej występują: komórki lokatorskie, miejsca postojowe i infrastruktura osiedlowa, w wynajmie częściej pojawiają się mieszkania “użytkowe”, gdzie te dodatki nie zawsze są formalnie przypisane.\n",
        "\n",
        "#### 3. Różnice między rankingiem liczbowym i procentowym\n",
        "- W rankingach liczbowych TOP miast dla każdej cechy niemal zawsze wygrywa Warszawa — co jest konsekwencją największej liczby ogłoszeń.\n",
        "- Dopiero wykresy procentowe pokazują realne różnice w strukturze zasobu:\n",
        "- hasParkingSpace: w wielu miastach udział `yes` jest zbliżony, ale są też wyraźne odstępstwa (np. w części miast udział jest zauważalnie wyższy niż w Warszawie).\n",
        "- hasBalcony: cecha jest ogólnie bardzo stabilna między największymi miastami (często ok. 55–65% `yes`), co sugeruje, że balkon jest w dużych ośrodkach standardem oferty, a różnice są raczej umiarkowane.\n",
        "- hasElevator: większe zróżnicowanie procentowe — w niektórych miastach udział `yes` jest zdecydowanie wyższy, co może odzwierciedlać większy udział zabudowy wielopiętrowej / nowszych budynków.\n",
        "- hasSecurity i hasStorageRoom: cechy rzadziej występujące, ale z większymi wahaniami między miastami\n",
        "\n",
        "#### 4. Różnice miejskie sugerują inny „profil budynków” i segmentację rynku\n",
        "- Miasta różnią się nie tylko cenami, ale też strukturą cech: w jednych relatywnie częściej występują windy, w innych komórki lokatorskie, a gdzie indziej miejsca parkingowe.\n",
        "- To wskazuje, że `city` jest zmienną silnie determinującą (nie tylko jako lokalizacja, ale też jako pośrednia informacja o typie zabudowy i standardu), a cechy `has*` mogą działać jako cechy doprecyzowujące segment wewnątrz miasta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba37db0",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "order_cities = df_sell.groupby('city')['price_per_m2'].median().sort_values(ascending=False).index\n",
        "\n",
        "sns.boxplot(x='city', y='price_per_m2', data=df_sell, order=order_cities, hue='city', palette=\"viridis\", legend=False)\n",
        "plt.title('Rozkład Ceny za m² w poszczególnych miastach')\n",
        "plt.xlabel('Miasto')\n",
        "plt.ylabel('Cena za m² (PLN)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e87cc7a",
      "metadata": {},
      "source": [
        "### Wnioski z wykresu rozkładu ceny za m² w poszczególnych miastach\n",
        "#### 1. Wyraźne różnice poziomu cen między miastami\n",
        "- Wykres pokazuje silny efekt lokalizacji (city effect): mediany cen za m² znacząco się różnią w zależności od miasta.\n",
        "- Warszawa ma najwyższą medianę i ogólnie najwyższy poziom cen za m² w całym zestawieniu. W kolejnej grupie znajdują się miasta typu Kraków oraz miasta Trójmiasta (Gdańsk / Gdynia), a następnie m.in. Wrocław.\n",
        "- Na drugim biegunie znajdują się miasta o najniższych medianach (np. Radom, Częstochowa), co wskazuje na zupełnie inną skalę rynkową niż w największych aglomeracjach.\n",
        "\n",
        "#### 2. Zmienność wewnątrz miast (IQR) sugeruje różną heterogeniczność rynku\n",
        "- Szerokość pudełek (IQR) jest w wielu miastach wyraźna, co oznacza, że nawet w obrębie jednego miasta występuje istotne zróżnicowanie cen za m².\n",
        "- Największe rynki (np. Warszawa, Kraków, Trójmiasto) zwykle mają większą rozpiętość rozkładu, co jest spójne z większą segmentacją: centrum vs peryferia, rynek premium vs standard, nowe inwestycje vs starsza zabudowa.\n",
        "\n",
        "#### 3. Liczne wartości odstające – obecność segmentu premium i długi ogon rozkładu\n",
        "- We wszystkich miastach widoczne są liczne outliery po stronie wysokich cen, szczególnie w największych ośrodkach. To sygnał, że rozkład jest prawoskośny (długi ogon w górę).\n",
        "- Najwięcej i najwyższe wartości odstające występują w miastach o najwyższych medianach, co sugeruje dobrze rozwinięty segment mieszkań premium (np. apartamenty w najlepszych lokalizacjach, nowe inwestycje, oferta pod inwestora)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3b6788e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _ppm2(df):\n",
        "    df = df.copy()\n",
        "    if \"price_per_m2\" not in df.columns:\n",
        "        df[\"price_per_m2\"] = df[\"price\"] / df[\"squareMeters\"]\n",
        "    return df\n",
        "\n",
        "def _gdf(df):\n",
        "    df = df.dropna(subset=[\"latitude\", \"longitude\", \"price_per_m2\"]).copy()\n",
        "    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\").to_crs(\"EPSG:3857\")\n",
        "\n",
        "def _waw(df):\n",
        "    return df[df[\"city\"].astype(str).str.strip().str.lower().eq(\"warszawa\")].copy()\n",
        "\n",
        "def _hex(ax, gdf, title, gridsize, mincnt):\n",
        "    hb = ax.hexbin(gdf.geometry.x, gdf.geometry.y, C=gdf[\"price_per_m2\"], reduce_C_function=np.median, gridsize=gridsize, mincnt=mincnt)\n",
        "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
        "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
        "    mx, my = (xmax - xmin) * 0.05, (ymax - ymin) * 0.05\n",
        "    ax.set_xlim(xmin - mx, xmax + mx); ax.set_ylim(ymin - my, ymax + my)\n",
        "    ax.set_axis_off(); ax.set_title(title, pad=10)\n",
        "    return hb\n",
        "\n",
        "sell, rent = _ppm2(df_sell), _ppm2(df_rent)\n",
        "g_sell_pl, g_rent_pl = _gdf(sell), _gdf(rent)\n",
        "g_sell_waw, g_rent_waw = _gdf(_waw(sell)), _gdf(_waw(rent))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 7))\n",
        "hb1 = _hex(ax[0], g_sell_pl, \"SELL (Polska): mediana price_per_m2 (min 10 / heks)\", 60, 10)\n",
        "hb2 = _hex(ax[1], g_rent_pl, \"RENT (Polska): mediana price_per_m2 (min 10 / heks)\", 60, 10)\n",
        "fig.colorbar(hb1, ax=ax[0], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
        "fig.colorbar(hb2, ax=ax[1], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 7))\n",
        "hb1 = _hex(ax[0], g_sell_waw, \"SELL (Warszawa): mediana price_per_m2 (min 10 / heks)\", 45, 10)\n",
        "hb2 = _hex(ax[1], g_rent_waw, \"RENT (Warszawa): mediana price_per_m2 (min 10 / heks)\", 45, 10)\n",
        "fig.colorbar(hb1, ax=ax[0], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
        "fig.colorbar(hb2, ax=ax[1], fraction=0.046, pad=0.04, label=\"mediana price_per_m2\")\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40878df0",
      "metadata": {},
      "source": [
        "### Wnioski z map heksowych price_per_m2 \n",
        "\n",
        "#### 1. Obraz dla Polski \n",
        "- Na mapach ogólnopolskich widać wyraźną polaryzację cen pomiędzy największymi rynkami a pozostałymi miastami. Dla sprzedaży  najwyższe wartości price_per_m2 koncentrują się w największych aglomeracjach, co jest spójne z intuicją rynkową: silny popyt, wyższe dochody oraz większy udział nowej zabudowy i atrakcyjnych lokalizacji.\n",
        "- Dla wynajmu  rozkład jest podobny (najdroższe ośrodki nadal dominują), ale skala wartości jest oczywiście inna. W praktyce widać, że zarówno w sprzedaży, jak i w najmie Warszawa dominuje.\n",
        "- Jednocześnie mapy potwierdzają, że analiza w skali kraju powinna być prowadzona z uwzględnieniem faktu, że porównujemy różne rynki lokalne, a nie jednorodny.\n",
        "\n",
        "#### 2. Obraz dla Warszawy \n",
        "- W Warszawie na mapie sprzedaży widać mocny gradient cenowy: najwyższe wartości price_per_m2 układają się w centralnej części miasta i w wybranych kierunkach tworzą spójne „pasma” podwyższonych cen, podczas gdy peryferia (oraz obszary o mniejszej liczbie ofert) częściej wykazują niższe mediany.\n",
        "- Na mapie wynajmu  wzorzec jest podobny w sensie geograficznym (również wyróżnia się rdzeń i obszary o wyższych stawkach), jednak struktura bywa bardziej rozproszona, co może wynikać z większej heterogeniczności ofert najmu (standard, metraż, segment rynku, krótkoterminowe ogłoszenia itp.).\n",
        "- Co istotne, oba wykresy  sugerują, że w Warszawie ceny nie rosną równomiernie „od centrum w kółko”, tylko tworzą układ, który może być powiązany z osiami dobrej dostępności komunikacyjnej.\n",
        "\n",
        "#### 3. Droższe lokalizacje wzdłuż M1\n",
        "- Na mapach Warszawy da się zauważyć układ podwyższonych cen, który luźno przypomina liniowy korytarz. Jedną z naturalnych interpretacji jest wpływ infrastruktury transportowej – w szczególności pierwszej linii metra (M1), która łączy północ–południe i przebiega przez kluczowe obszary miejskie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0fabf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "GRIDSIZE_WAW, MINCNT = 80, 1\n",
        "FEATURES = [\"hasBalcony\", \"hasElevator\", \"hasSecurity\", \"hasParkingSpace\", \"hasStorageRoom\"]\n",
        "\n",
        "_waw = lambda df: df[df[\"city\"].astype(str).str.strip().str.lower().eq(\"warszawa\")].copy()\n",
        "_gdf = lambda df: gpd.GeoDataFrame(df.dropna(subset=[\"latitude\",\"longitude\"]).copy(),\n",
        "                                  geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
        "                                  crs=\"EPSG:4326\").to_crs(\"EPSG:3857\")\n",
        "\n",
        "def _hex(ax, gdf, title):\n",
        "    hb = ax.hexbin(gdf.geometry.x, gdf.geometry.y, gridsize=GRIDSIZE_WAW, mincnt=MINCNT)\n",
        "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
        "    xmin, ymin, xmax, ymax = gdf.total_bounds\n",
        "    mx, my = (xmax-xmin)*0.05, (ymax-ymin)*0.05\n",
        "    ax.set_xlim(xmin-mx, xmax+mx); ax.set_ylim(ymin-my, ymax+my)\n",
        "    ax.set_axis_off(); ax.set_title(title, pad=10)\n",
        "    return hb\n",
        "\n",
        "df = _waw(pd.concat([df_sell, df_rent], ignore_index=True))\n",
        "has_cols = [c for c in df.columns if c.startswith(\"has\")]\n",
        "for c in has_cols: df[c] = df[c].where(df[c].isin([\"yes\",\"no\"]))\n",
        "features = [c for c in (FEATURES or has_cols) if c in df.columns]\n",
        "gdf = _gdf(df)\n",
        "\n",
        "ncols, nrows = 2, int(np.ceil(len(features)/2))\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(16, max(6, nrows*5)))\n",
        "axes = np.array(axes).reshape(-1)\n",
        "\n",
        "for i, feat in enumerate(features):\n",
        "    sub = gdf[gdf[feat].eq(\"yes\")]\n",
        "    hb = _hex(axes[i], sub, f\"Warszawa (SELL+RENT) | {feat}=yes: intensywność występowania (hexbin count)\")\n",
        "    fig.colorbar(hb, ax=axes[i], fraction=0.046, pad=0.04, label=\"liczba ofert w heksie\")\n",
        "\n",
        "for j in range(i+1, len(axes)): axes[j].axis(\"off\")\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c5c925",
      "metadata": {},
      "source": [
        "### Wnioski z map intensywności występowania cech has* w Warszawie\n",
        "\n",
        "#### 1. Co dokładnie pokazują mapy\n",
        "- Mapy przedstawiają zagęszczenie ofert (hexbin count) spełniających warunek `hasX = yes`, czyli gdzie dana cecha (np. balkon, winda, ochrona) występuje w ogłoszeniach.\n",
        "\n",
        "#### 2. Balkon (hasBalcony)\n",
        "- Balkon jest cechą szeroko rozpowszechnioną – mapa jest stosunkowo „pełna”, co sugeruje, że w wielu częściach Warszawy oferty z balkonem pojawiają się regularnie.\n",
        "- Największe natężenia występują w obszarach o wysokiej aktywności rynku (duża liczba ogłoszeń), co jest spójne z tym, że balkon jest standardem oferty w wielu segmentach\n",
        "\n",
        "#### 3. Winda (hasElevator)\n",
        "- hasElevator generuje wyraźnie silniejsze ogniska koncentracji niż balkon, co wskazuje na duży wolumen ofert w zabudowie, gdzie winda jest typowym wyposażeniem.\n",
        "- Przestrzennie rozkład jest zgodny z intuicją urbanistyczną: winda częściej pojawia się tam, gdzie dominuje zabudowa wielopiętrowa lub nowsze inwestycje mieszkaniowe. Jednocześnie brak widocznych „punktowych” koncentracji sugeruje, że to cecha strukturalna dla typów budynków, a nie atrybut związany z pojedynczymi mikro-lokalizacjami.\n",
        "\n",
        "#### 4. Ochrona (hasSecurity)\n",
        "- hasSecurity jest cechą zdecydowanie rzadszą – mapa zawiera mniej silnych koncentracji, a wiele heksów ma niską intensywność.\n",
        "- W praktyce oznacza to, że ochrona w ogłoszeniach jest bardziej charakterystyczna dla wybranych inwestycji/kompleksów (np. osiedla zamknięte, segment premium), a nie dla całego rynku.\n",
        "\n",
        "#### 5. Miejsce parkingowe (hasParkingSpace)\n",
        "- Cecha występuje częściej niż ochrona, ale jej natężenia są bardziej selektywne niż w przypadku balkonu.\n",
        "- Przestrzennie może to odzwierciedlać większy udział inwestycji, w których parking jest przypisany do lokalu (garaże podziemne, miejsca postojowe), co bywa częstsze w nowszej zabudowie i określonych typach osiedli.\n",
        "\n",
        "#### 6. Komórka lokatorska (hasStorageRoom)\n",
        "- hasStorageRoom wykazuje stosunkowo szeroki zasięg, ale intensywności są wyraźnie niższe niż przy windzie.\n",
        "- To sugeruje, że komórka lokatorska pojawia się jako cecha istotna, lecz mniej „domyślna” niż balkon czy winda – może być mocniej związana z konkretnymi typami budynków i standardem inwestycji."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3182a2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "GRIDSIZE_WAW, MINCNT, TOP_Q, N_SEGMENTS, SMOOTH_WINDOW = 80, 10, 0.85, 35, 5\n",
        "\n",
        "_ppm2 = lambda df: df.assign(price_per_m2=df[\"price\"]/df[\"squareMeters\"]) if \"price_per_m2\" not in df.columns else df.copy()\n",
        "_waw  = lambda df: df[df[\"city\"].astype(str).str.strip().str.lower().eq(\"warszawa\")].copy()\n",
        "_gdf  = lambda df: gpd.GeoDataFrame(df.dropna(subset=[\"latitude\",\"longitude\",\"price_per_m2\"]).copy(),\n",
        "                                   geometry=gpd.points_from_xy(df.longitude, df.latitude),\n",
        "                                   crs=\"EPSG:4326\").to_crs(\"EPSG:3857\")\n",
        "\n",
        "def _path(centers, n=N_SEGMENTS, w=SMOOTH_WINDOW):\n",
        "    m = centers.mean(axis=0); X = centers - m\n",
        "    v = np.linalg.eigh(np.cov(X.T))[1][:, -1]; v = v / np.linalg.norm(v)\n",
        "    t = X @ v; edges = np.linspace(t.min(), t.max(), n + 1)\n",
        "    pts = np.array([centers[(t>=a) & (t<b)].mean(axis=0) for a,b in zip(edges[:-1], edges[1:]) if ((t>=a) & (t<b)).any()])\n",
        "    if w > 1 and len(pts) >= 3:\n",
        "        k, p = w, w//2\n",
        "        pad = np.pad(pts, ((p,p),(0,0)), mode=\"edge\")\n",
        "        pts = np.array([pad[i:i+k].mean(axis=0) for i in range(len(pts))])\n",
        "    return pts\n",
        "\n",
        "def plot_line_guess(df, label):\n",
        "    g = _gdf(_waw(_ppm2(df)))\n",
        "    fig, ax = plt.subplots(figsize=(10, 9))\n",
        "    hb = ax.hexbin(g.geometry.x, g.geometry.y, C=g[\"price_per_m2\"], reduce_C_function=np.median, gridsize=GRIDSIZE_WAW, mincnt=MINCNT)\n",
        "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
        "    xmin, ymin, xmax, ymax = g.total_bounds\n",
        "    mx, my = (xmax-xmin)*0.05, (ymax-ymin)*0.05\n",
        "    ax.set_xlim(xmin-mx, xmax+mx); ax.set_ylim(ymin-my, ymax+my); ax.set_axis_off()\n",
        "\n",
        "    centers = hb.get_offsets()\n",
        "    vals = np.asarray(hb.get_array())\n",
        "    if hasattr(vals, \"filled\"): vals = vals.filled(np.nan)\n",
        "\n",
        "    top = centers[vals >= np.nanquantile(vals, TOP_Q)]\n",
        "    pts = _path(top)\n",
        "\n",
        "    ax.scatter(top[:,0], top[:,1], s=10, alpha=0.9)\n",
        "    if len(pts) >= 2: ax.plot(pts[:,0], pts[:,1], linewidth=3)\n",
        "\n",
        "    ax.set_title(f\"{label} | Warszawa: przybliżony przebieg osi najwyższych median price_per_m2 (top {(1-TOP_Q)*100:.0f}%)\", pad=12)\n",
        "    plt.colorbar(hb, ax=ax, fraction=0.046, pad=0.04, label=\"mediana price_per_m2 (PLN/m²)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_line_guess(df_sell, \"SELL\")\n",
        "plot_line_guess(df_rent, \"RENT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fff07526",
      "metadata": {},
      "source": [
        "### Wnioski z próby odtworzenia przebiegu linii metra na podstawie cen w Warszawie\n",
        "\n",
        "#### 1. Cel i założenie eksperymentu\n",
        "- W tej części analizy podjęto próbę przewidzenia przybliżonego przebiegu pierwszej linii metra (M1) w Warszawie, korzystając wyłącznie z informacji zawartych w danych ofertowych.\n",
        "- Założeniem było, że dostęp do metra istotnie wpływa na atrakcyjność lokalizacji, a więc może przekładać się na wyższe średnie/medianowe ceny za m² w obszarach położonych wzdłuż osi transportowej.\n",
        "\n",
        "#### 2. Metoda (intuicja)\n",
        "- Na siatce heksów obliczono medianę price_per_m2 i wybrano obszary o najwyższych wartościach (top 15% heksów).\n",
        "- Następnie na podstawie geometrii tych obszarów wyznaczono ciągłą linię, która ma reprezentować dominującą oś przestrzenną wysokich cen.\n",
        "- Kluczowe jest to, że procedura nie korzystała z żadnych danych o transporcie (brak lokalizacji stacji, przebiegu torów, przystanków), więc wynik jest wyłącznie wnioskowaniem pośrednim na podstawie rozkładu cen.\n",
        "\n",
        "#### 3. Wynik: zgodność z rzeczywistym przebiegiem M1\n",
        "- Otrzymana linia jest mocno zbliżona do rzeczywistego przebiegu M1, zwłaszcza w wariancie dla sprzedaży (SELL). Widoczna jest dominująca orientacja północ–południe i przebieg przez obszary o najwyższych medianach cen.\n",
        "- Dla wynajmu (RENT) zgodność również jest widoczna, ale rezultat jest mniej jednoznaczny i bardziej podatny na lokalne odchylenia\n",
        "\n",
        "#### 4. Odchylenie na południu (kierunek Wilanowa)\n",
        "- W obu wariantach zauważalne jest, że dolny fragment wyznaczonej trasy odchyla się bardziej w stronę Wilanowa niż rzeczywista M1.\n",
        "- To prawdopodobnie efekt tego, że Wilanów jest obszarem o relatywnie wysokich cenach, a metoda oparta na cenie/m² traktuje takie dzielnice jako “silny sygnał”, mimo że nie wynika on bezpośrednio z przebiegu M1.\n",
        "- Ten element pokazuje ograniczenie podejścia: rozkład cen odzwierciedla jednocześnie wiele czynników (centrum, standard zabudowy, prestiż lokalizacji, infrastruktura), a nie wyłącznie dostępność metra."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d7a48a",
      "metadata": {},
      "source": [
        "## 5. Statystyki liczbowe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86d3499",
      "metadata": {},
      "outputs": [],
      "source": [
        "PCTS = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]\n",
        "\n",
        "def ensure_price_per_m2(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"price_per_m2\" not in df.columns and {\"price\", \"squareMeters\"}.issubset(df.columns):\n",
        "        df[\"price_per_m2\"] = df[\"price\"] / df[\"squareMeters\"]\n",
        "    return df\n",
        "\n",
        "def stats_table(df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
        "    df = ensure_price_per_m2(df)\n",
        "\n",
        "    wanted = [\"price\", \"squareMeters\", \"rooms\", \"centreDistance\", \"buildYear\", \"price_per_m2\"]\n",
        "    cols = [c for c in wanted if c in df.columns]\n",
        "\n",
        "    desc = df[cols].describe(percentiles=PCTS).T  \n",
        "    desc.insert(0, \"dataset\", label)\n",
        "\n",
        "    return desc.round(2)\n",
        "\n",
        "stats_sell = stats_table(df_sell, \"SELL\")\n",
        "stats_rent = stats_table(df_rent, \"RENT\")\n",
        "\n",
        "display(stats_sell)\n",
        "display(stats_rent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e290a0a6",
      "metadata": {},
      "source": [
        "### Wnioski z statystyk opisowych\n",
        "\n",
        "#### 1. Skala zbioru i kompletność danych\n",
        "- Zbiór sprzedaży jest znacznie większy: 195 568 ofert vs 70 847 ofert dla wynajmu.\n",
        "- buildYear ma braki: dostępny dla ok. 163 352/195 568 (~83,5%) w SELL oraz 51 165/70 847 (~72,2%) w RENT. W dalszej analizie/modelowaniu trzeba to uwzględnić\n",
        "\n",
        "#### 2. Ceny – poziom i rozkład\n",
        "SELL (cena całkowita):\n",
        "- Mediana ceny to ok. 699 tys. PLN, a 75% ofert mieści się do 930 tys. PLN.\n",
        "- Rozkład jest wyraźnie prawoskośny (długi ogon): 95% ofert jest poniżej ~1,59 mln PLN, ale maksimum sięga 3,25 mln PLN – widać segment premium/outliery.\n",
        "\n",
        "RENT (czynsz miesięczny):\n",
        "- Mediana czynszu to ok. 3 100 PLN, a 75% ofert do 4 490 PLN.\n",
        "- Również silna prawoskośność: 95% poniżej 8 500 PLN, a maksimum 23 000 PLN wskazuje na segment premium / potencjalne obserwacje odstające.\n",
        "\n",
        "#### 3. Cena za m² (price_per_m2) – kluczowa metryka porównawcza\n",
        "- SELL: mediana price_per_m2 ≈ 12 979 PLN/m², a 90 percentyl ≈ 20 388 PLN/m². Zakres (min–max) jest szeroki (3 000 → 32 097 PLN/m²), co sugeruje duże zróżnicowanie lokalizacji i standardu.\n",
        "- RENT: mediana price_per_m2 ≈ 65,96 PLN/m² (miesięcznie), 90 percentyl ≈ 99,16 PLN/m². Maksimum 189,47 PLN/m² to również sygnał segmentu premium.\n",
        "\n",
        "#### 4. Metraż i liczba pokoi – „typowa” oferta\n",
        "- Metraże są zbliżone, ale SELL jest minimalnie większy:\n",
        "- SELL: mediana 54,6 m², 75% do 68,6 m²\n",
        "- RENT: mediana 50 m², 75% do 64 m²\n",
        "- Struktura pokoi:\n",
        "- SELL: mediana 3 pokoje (25–75%: 2–3)\n",
        "- RENT: mediana 2 pokoje (25–75%: 2–3)\n",
        "To pasuje do intuicji: najem częściej dotyczy mniejszych lokali.\n",
        "\n",
        "#### 5. Odległość od centrum (centreDistance)\n",
        "- RENT jest przeciętnie bliżej centrum: średnia 3,86 km vs 4,35 km w SELL; także mediana jest niższa (3,38 km vs 3,98 km).\n",
        "- To wspiera tezę, że rynek najmu jest bardziej skoncentrowany w lokalizacjach centralnych/okołocentralnych, gdzie popyt najemców jest najwyższy.\n",
        "\n",
        "#### 6. Rok budowy (buildYear) – różnice między rynkami\n",
        "- RENT ma wyraźnie młodszy zasób w danych: mediana 2001 vs 1994 w SELL.\n",
        "- W RENT 75 percentyl to 2020, podczas gdy w SELL 2016. Może to wynikać z większej reprezentacji nowej zabudowy w ofertach najmu (np. inwestycje kupowane pod wynajem)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74954c7d",
      "metadata": {},
      "source": [
        "### Analiza Rozkładu Zmiennej Celu (Skośność)\n",
        "\n",
        "Modele regresji liniowej najlepiej działają, gdy zmienna celu ma rozkład zbliżony do normalnego (Krzywa Gaussa).\n",
        "Poniżej porównujemy rozkład surowej ceny (`price`) oraz jej logarytmu (`log_price`).\n",
        "\n",
        "* **Skośność (Skewness):** Miara asymetrii rozkładu.\n",
        "    * Wartość > 1 oznacza silną asymetrię prawostronną (dużo tanich, mało drogich).\n",
        "    * Wartość bliska 0 oznacza rozkład symetryczny (idealny dla modelu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95cbf610",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def analyze_price_skewness(df, dataset_name, target_col='price'):\n",
        "    # Wykonujemy kopie\n",
        "    df_dist = df.copy()\n",
        "\n",
        "    # Obliczenie logarytmu\n",
        "    # log1p to log(x + 1) - bezpieczne dla wartości bliskich 0\n",
        "    df_dist['log_price'] = np.log1p(df_dist[target_col])\n",
        "\n",
        "    # Obliczenie skośności (Skewness)\n",
        "    skew_raw = df_dist[target_col].skew()\n",
        "    skew_log = df_dist['log_price'].skew()\n",
        "\n",
        "    # Wizualizacja \"Przed i Po\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Rozkład surowy (PLN)\n",
        "    sns.histplot(df_dist[target_col], bins=50, kde=True, color='skyblue', ax=axes[0])\n",
        "    axes[0].set_title(f'Rozkład Ceny (Surowy) - {dataset_name}\\nSkośność: {skew_raw:.2f} (Prawoskośny)', fontsize=12)\n",
        "    axes[0].set_xlabel('Cena (PLN)')\n",
        "    axes[0].axvline(df_dist[target_col].mean(), color='red', linestyle='--', label='Średnia')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Rozkład logarytmiczny\n",
        "    sns.histplot(df_dist['log_price'], bins=50, kde=True, color='purple', ax=axes[1])\n",
        "    \n",
        "    # Dopasowanie idealnej krzywej normalnej dla porównania\n",
        "    xmin, xmax = axes[1].get_xlim()\n",
        "    x = np.linspace(xmin, xmax, 100)\n",
        "    mu, std = df_dist['log_price'].mean(), df_dist['log_price'].std()\n",
        "    p = norm.pdf(x, mu, std)\n",
        "    \n",
        "    # Skalowanie krzywej normalnej do wysokości histogramu\n",
        "    axes[1].plot(x, p * len(df_dist) * (xmax - xmin)/50, 'k', linewidth=2, label='Rozkład Normalny')\n",
        "\n",
        "    axes[1].set_title(f'Rozkład Logarytmu Ceny - {dataset_name}\\nSkośność: {skew_log:.2f} (Zbliżona do zera)', fontsize=12)\n",
        "    axes[1].set_xlabel('Log(Cena)')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "analyze_price_skewness(df_sell, \"SPRZEDAŻ\")\n",
        "analyze_price_skewness(df_rent, \"WYNAJEM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "944c725c",
      "metadata": {},
      "source": [
        "### Wnioski z analizy rozkładu zmiennej celu:\n",
        "\n",
        "Analiza rozkładu cen dla obu rynków (sprzedaż i wynajem) wykazała **silną prawoskośność (skośność > 1)**. Oznacza to, że choć większość ofert mieści się w niższych przedziałach cenowych, istnieje długi \"ogon\" bardzo drogich nieruchomości luksusowych, który drastycznie zaburza średnią i może negatywnie wpływać na stabilność modeli.\n",
        "\n",
        "Wykresy jednoznacznie pokazują, że zastosowanie transformacji logarytmicznej (`np.log1p`) skutecznie normalizuje dane – rozkład \"fioletowy\" przypomina dzwon Gaussa, a jego skośność spada do poziomu bliskiego 0. Jest to ostateczne potwierdzenie, że **model powinien przewidywać `log_price`**, a nie surową kwotę, co jest kluczowe szczególnie dla poprawności działania modeli statystycznych."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0c97a6",
      "metadata": {},
      "source": [
        "## 6. Badanie Zależności i Korelacje\n",
        "\n",
        "W tym kroku identyfikujemy zmienne, które mają największy wpływ na cenę mieszkania. Analiza obejmuje:\n",
        "1.  **Analizę `log_price`:** Weryfikację, czy zlogarytmowanie ceny (zmienna celu) zwiększa siłę korelacji (co jest typowe dla rozkładów prawoskośnych).\n",
        "2.  **Macierz korelacji:** Sprawdzenie siły zależności liniowej (Pearson) między cechami numerycznymi a zmienną celu.\n",
        "3.  **Wizualizację trendów:** Wykresy punktowe badające relację Cena vs Metraż (dla kluczowych miast) oraz wpływ lokalizacji na cenę za m²."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40752623",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_correlations(df, dataset_name, method='pearson'):\n",
        "    df_corr = df.copy()\n",
        "    df_corr['log_price'] = np.log1p(df_corr['price'])\n",
        "    \n",
        "    # Wybór zmiennych numerycznych do analizy\n",
        "    target_cols = ['price', 'log_price', 'price_per_m2']\n",
        "    feature_cols = [\n",
        "        'squareMeters', 'rooms', 'floor', 'floorCount', 'buildYear', \n",
        "        'centreDistance', 'poiCount', 'schoolDistance', 'clinicDistance', \n",
        "        'restaurantDistance', 'kindergartenDistance'\n",
        "    ]\n",
        "    available_cols = target_cols + [c for c in feature_cols if c in df_corr.columns]\n",
        "    \n",
        "    # Obliczenie macierzy korelacji\n",
        "    corr_matrix = df_corr[available_cols].corr(method=method)\n",
        "    \n",
        "    # Wizualizacja - Heatmapa\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # Ukrywamy górny trójkąt (duplikaty)\n",
        "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', \n",
        "                mask=mask, vmin=-1, vmax=1, center=0, cbar_kws={\"shrink\": .8})\n",
        "    plt.title(f'Macierz Korelacji ({dataset_name})')\n",
        "    plt.show()\n",
        "    \n",
        "    # Ranking najważniejszych cech (Tabela)\n",
        "    print(f\"\\nTOP 10 Korelacji ze zmiennymi celu ({dataset_name}):\")\n",
        "    \n",
        "    # Pobieramy korelacje dla log_price i price_per_m2\n",
        "    rank_log = corr_matrix['log_price'].drop(target_cols, errors='ignore').abs().sort_values(ascending=False).head(10)\n",
        "    rank_m2 = corr_matrix['price_per_m2'].drop(target_cols, errors='ignore').abs().sort_values(ascending=False).head(10)\n",
        "    \n",
        "    # Funkcje pomocnicze do budowy tabeli\n",
        "    idx1, val1 = rank_log.index.tolist(), corr_matrix.loc[rank_log.index, 'log_price'].values\n",
        "    idx2, val2 = rank_m2.index.tolist(), corr_matrix.loc[rank_m2.index, 'price_per_m2'].values\n",
        "    max_len = max(len(idx1), len(idx2))\n",
        "    \n",
        "    def pad(l, size, fill): return l + [fill] * (size - len(l))\n",
        "\n",
        "    ranking_df = pd.DataFrame({\n",
        "        'Cecha (log_price)': pad(idx1, max_len, '-'),\n",
        "        'Korelacja (log_price)': pad(list(val1), max_len, np.nan),\n",
        "        ' | ': ['|'] * max_len,\n",
        "        'Cecha (price_per_m2)': pad(idx2, max_len, '-'),\n",
        "        'Korelacja (price_per_m2)': pad(list(val2), max_len, np.nan)\n",
        "    })\n",
        "    \n",
        "    # Wyświetlenie sformatowanej tabeli\n",
        "    display(ranking_df.style.background_gradient(\n",
        "        cmap='coolwarm', \n",
        "        subset=['Korelacja (log_price)', 'Korelacja (price_per_m2)'], \n",
        "        vmin=-1, vmax=1\n",
        "    ).format(\"{:.3f}\", subset=['Korelacja (log_price)', 'Korelacja (price_per_m2)']))\n",
        "\n",
        "# Wywołanie dla wyczyszczonych danych\n",
        "analyze_correlations(df_sell, \"SPRZEDAŻ\")\n",
        "analyze_correlations(df_rent, \"WYNAJEM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ae2c01",
      "metadata": {},
      "source": [
        "### Wnioski z analizy korelacji:\n",
        "\n",
        "Na podstawie macierzy korelacji dla obu podzbiorów (sprzedaży i wynajmu) można sformułować następujące spostrzeżenia dotyczące czynników cenotwórczych:\n",
        "\n",
        "1.  **Fundamentalna rola metrażu (Różnice rynkowe):**\n",
        "    Najsilniejszym predyktorem ceny całkowitej jest powierzchnia (`squareMeters`). Co ciekawe, korelacja ta jest **silniejsza dla rynku wynajmu (0.744)** niż dla sprzedaży (0.608). Sugeruje to, że ceny najmu są bardziej \"sztywno\" powiązane z wielkością lokalu, podczas gdy na cenę sprzedaży większy wpływ mogą mieć inne czynniki (np. standard wykończenia, prestiż dzielnicy, stan prawny), których proste cechy numeryczne nie w pełni oddają.\n",
        "\n",
        "2.  **Lokalizacja - \"Usługi ważniejsze niż Centrum\":**\n",
        "    W przypadku ceny za m² (`price_per_m2`), silniejszą korelację niż sama odległość od centrum (`centreDistance`) wykazuje liczba punktów użyteczności publicznej (`poiCount`: ~0.23) oraz bliskość restauracji (`restaurantDistance`: ~-0.18).\n",
        "    * **Wniosek:** Dla wartości nieruchomości ważniejsza od geometrycznego środka miasta jest **funkcjonalność okolicy** (dostęp do usług, gastronomii). Potwierdza to koncepcję miasta policentrycznego.\n",
        "\n",
        "3.  **Wiek budynku (Kupno vs Wynajem):**\n",
        "    Zmienna `buildYear` odgrywa znacznie większą rolę w sprzedaży (korelacja 0.204 z ceną za m²) niż w wynajmie (tylko 0.087).\n",
        "    * **Interpretacja:** Kupujący traktują mieszkanie jako inwestycję długoterminową, więc nowsze budownictwo jest wyceniane znacznie wyżej (mniejsze ryzyko remontów, wyższa efektywność energetyczna). Dla najemcy rok budowy jest drugorzędny – liczy się obecny standard i lokalizacja.\n",
        "\n",
        "4.  **Efekt \"Hurtu\" (Korelacja ujemna):**\n",
        "    W danych sprzedażowych widoczna jest ujemna korelacja między metrażem a ceną za m² (`-0.103`). Jest to klasyczne zjawisko rynkowe: mniejsze lokale (kawalerki) mają najwyższą cenę jednostkową, a wraz ze wzrostem metrażu cena za m² spada. W danych najmu ta zależność zanika lub jest lekko dodatnia, co sugeruje inną dynamikę popytu na duże mieszkania na wynajem.\n",
        "\n",
        "5.  **Uzasadnienie dla transformacji logarytmicznej:**\n",
        "    Analiza potwierdza, że zmienna `log_price` wykazuje wyższe wartości korelacji z cechami (np. 0.608) niż surowa cena. Potwierdza to zasadność trenowania modeli na logarytmie ceny w celu zlinearyzowania zależności wykładniczych.\n",
        "\n",
        "6.  **Problem współliniowości:**\n",
        "    Bardzo wysoka korelacja między `squareMeters` a `rooms` (często > 0.8) wskazuje na redundancję tych cech. W modelach wrażliwych na współliniowość (np. Regresja Liniowa) może to destabilizować wagi cech."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41bc6f01",
      "metadata": {},
      "source": [
        "### Analiza porównawcza rynków lokalnych\n",
        "\n",
        "Rynek nieruchomości w Polsce nie jest jednorodny. Aby zweryfikować hipotezę, że cena mieszkania zależy nie tylko od jego metrażu, ale także od miasta, w którym się znajduje, przeprowadzamy analizę dla 4 największych rynków: Warszawy, Krakowa, Wrocławia i Poznania.\n",
        "\n",
        "Wykorzystujemy wykres punktowy z naniesioną linią regresji (`lmplot`), rozbity na panele. Pozwoli to porównać nachylenie krzywych cenowych – im bardziej stroma linia, tym droższy jest każdy kolejny metr kwadratowy w danym mieście."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e652a9db",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_cities_keys = ['warszawa', 'krakow', 'wroclaw', 'poznan'] \n",
        "\n",
        "# Słownik do mapowania nazw na ładne etykiety z polskimi znakami\n",
        "city_labels = {\n",
        "    'warszawa': 'Warszawa', \n",
        "    'krakow': 'Kraków', \n",
        "    'wroclaw': 'Wrocław', \n",
        "    'poznan': 'Poznań'\n",
        "}\n",
        "\n",
        "# Tworzymy kopię danych tylko dla danych miast\n",
        "df_plot = df_sell[df_sell['city'].isin(top_cities_keys)].copy()\n",
        "df_plot['city_label'] = df_plot['city'].map(city_labels)\n",
        "\n",
        "g = sns.lmplot(\n",
        "    data=df_plot, \n",
        "    x='squareMeters', \n",
        "    y='price', \n",
        "    col='city_label',   \n",
        "    col_wrap=2,         \n",
        "    hue='city_label',   \n",
        "    height=4, \n",
        "    aspect=1.5,\n",
        "    scatter_kws={'alpha': 0.3, 's': 15}, \n",
        "    line_kws={'color': '#333333'}      \n",
        ")\n",
        "g.figure.suptitle('Zależność Ceny całkowitej od Metrażu w największych miastach', y=1.02, fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4f360a",
      "metadata": {},
      "source": [
        "### Wnioski z analizy trendów w miastach (Cena vs Metraż):\n",
        "\n",
        "1.  **Potwierdzenie liniowości relacji:**\n",
        "    We wszystkich analizowanych aglomeracjach (Warszawa, Kraków, Wrocław, Poznań) obserwujemy silną, dodatnią korelację liniową. Punkty układają się w zwarte \"chmury\" wzdłuż linii regresji.\n",
        "\n",
        "2.  **Zróżnicowanie cen krańcowych (Nachylenie prostej):**\n",
        "    Kluczową obserwacją jest różnica w kącie nachylenia linii trendu (współczynnik kierunkowy).\n",
        "    * W **Warszawie** linia jest najbardziej stroma. Oznacza to, że każdy dodatkowy metr kwadratowy powierzchni podnosi cenę końcową znacznie bardziej niż w pozostałych miastach.\n",
        "    * W **Poznaniu** linia jest bardziej płaska, co sugeruje mniejszą wrażliwość ceny całkowitej na wzrost metrażu.\n",
        "    * **Implikacja dla modelu:** Model nie może traktować metrażu uniwersalnie. Konieczne jest uwzględnienie interakcji między zmiennymi, który \"zrozumie\", że 50 m² w Warszawie to inna klasa cenowa niż 50 m² we Wrocławiu.\n",
        "\n",
        "3.  **Heteroskedastyczność (Zmienność wariancji):**\n",
        "    Warto zauważyć, że im większy metraż, tym bardziej punkty są rozproszone wokół linii (chmura się rozszerza). Oznacza to, że ceny małych mieszkań są bardzo przewidywalne, natomiast w segmencie dużych apartamentów wariancja cen jest ogromna (zależą one mocniej od standardu wykończenia niż tylko od powierzchni)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23a0dee2",
      "metadata": {},
      "source": [
        "### Analiza wpływu lokalizacji (Mapa gęstości)\n",
        "\n",
        "Drugim kluczowym czynnikiem cenotwórczym jest odległość od centrum (`centreDistance`). Ponieważ zbiór danych jest duży, zwykły wykres punktowy byłby nieczytelny (problem nakładania się punktów).\n",
        "\n",
        "Zamiast tego stosujemy wykres heksagonalny (hexbin plot), który działa jak mapa termiczna.\n",
        "* **Oś X:** Odległość od centrum (km).\n",
        "* **Oś Y:** Cena za m² (PLN).\n",
        "* **Kolor:** Liczba ofert w danym obszarze (skala logarytmiczna).\n",
        "\n",
        "Dzięki temu zobaczymy nie tylko trend cenowy, ale także strukturę podaży – w jakiej odległości od centrum dostępnych jest najwięcej mieszkań."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf95ee2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Odsiewamy skrajne wartości (outliery) dla czytelności wykresu\n",
        "df_hex = df_sell[\n",
        "    (df_sell['centreDistance'] < 15) & \n",
        "    (df_sell['price_per_m2'] < 35000)\n",
        "]\n",
        "\n",
        "hb = plt.hexbin(\n",
        "    df_hex['centreDistance'], \n",
        "    df_hex['price_per_m2'], \n",
        "    gridsize=40,    \n",
        "    cmap='inferno', \n",
        "    mincnt=1,       \n",
        "    bins='log'     \n",
        ")\n",
        "\n",
        "cb = plt.colorbar(hb, label='Liczba ofert (skala log)')\n",
        "plt.title('Gęstość ofert: Cena za m² vs Odległość od centrum (Cała Polska)')\n",
        "plt.xlabel('Odległość od centrum (km)')\n",
        "plt.ylabel('Cena za m² (PLN)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d93c24e3",
      "metadata": {},
      "source": [
        "### Wnioski z analizy wpływu lokalizacji (Cena za m² vs Odległość od centrum):\n",
        "\n",
        "1.  **Nieliniowa dynamika spadku cen (Krzywa renty gruntowej):**\n",
        "    Wykres ujawnia charakterystyczny kształt litery \"L\" (lub rozkładu hiperbolicznego $1/x$).\n",
        "    * **Strefa Centrum (0-3 km):** Obserwujemy ekstremalnie wysokie ceny i ogromną wariancję (pionowy słupek po lewej stronie). Tutaj lokalizacja generuje \"premię za centralność\", która drastycznie winduje ceny luksusowych apartamentów.\n",
        "    * **Strefa Miejska (3-10 km):** Następuje gwałtowny spadek cen, po czym wykres zaczyna się wypłaszczać.\n",
        "    * **Peryferia (>10 km):** Ceny stabilizują się na stałym, niższym poziomie. Dalsze oddalanie się od centrum (np. z 12 na 15 km) ma już minimalny wpływ na spadek ceny.\n",
        "    * **Implikacja dla modelu:** Zastosowanie prostej korelacji liniowej dla cechy `centreDistance` będzie błędem (niedoszacuje cen w centrum i przeszacuje na peryferiach). Lepiej sprawdzi się model nieliniowy, np. k-NN.\n",
        "\n",
        "2.  **Koncentracja rynku (Analiza gęstości):**\n",
        "    Najjaśniejsze obszary na mapie (żółte/pomarańczowe heksagony) wskazują na \"masowy\" segment rynku.\n",
        "    * Największa podaż mieszkań występuje w pasie **3–8 km od centrum**.\n",
        "    * Ceny w tym segmencie są relatywnie jednorodne i skupione wokół średniej rynkowej (np. 10–15 tys. PLN/m²). To tutaj modele predykcyjne powinny osiągać najwyższą skuteczność ze względu na dużą liczbę próbek uczących i mniejszy rozrzut cenowy niż w ścisłym centrum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea60f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. PRZYGOTOWANIE DANYCH ---\n",
        "def get_preprocessor(numeric_features, categorical_features):\n",
        "    \"\"\"Tworzy i zwraca obiekt ColumnTransformer do przetwarzania danych.\"\"\"\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "    return preprocessor\n",
        "\n",
        "def prepare_data(df, city_filter=None, exclude_city=None, target_col='price'):\n",
        "    \"\"\"Filtruje dane, wybiera cechy, logarytmuje cel i dzieli na train/test.\"\"\"\n",
        "    data = df.copy()\n",
        "    \n",
        "    # Filtrowanie miastaf\n",
        "    if city_filter:\n",
        "        data = data[data['city'] == city_filter]\n",
        "\n",
        "    if exclude_city:\n",
        "        data = data[data['city'] != exclude_city]\n",
        "    \n",
        "    # Wybór cech\n",
        "    numeric_features = ['squareMeters', 'rooms', 'floor', 'buildYear', 'centreDistance', 'poiCount']\n",
        "    categorical_features = ['city'] if city_filter is None else []\n",
        "    \n",
        "    # Bezpieczny wybór tylko istniejących kolumn\n",
        "    numeric_features = [c for c in numeric_features if c in data.columns]\n",
        "    \n",
        "    X = data[numeric_features + categorical_features]\n",
        "    y = data[target_col]\n",
        "    \n",
        "    # Logarytmowanie celu\n",
        "    y_log = np.log1p(y)\n",
        "    \n",
        "    # Podział na zbiór treningowy i testowy\n",
        "    X_train, X_test, y_train_log, y_test_log = train_test_split(\n",
        "        X, y_log, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    return X_train, X_test, y_train_log, y_test_log, numeric_features, categorical_features\n",
        "\n",
        "# --- 2. FUNKCJE MODELI ---\n",
        "def train_linear_regression(X_train, y_train, preprocessor):\n",
        "    \"\"\"Trenuje model Regresji Liniowej.\"\"\"\n",
        "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', LinearRegression())])\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def train_decision_tree(X_train, y_train, preprocessor):\n",
        "    \"\"\"Trenuje model Drzewa Decyzyjnego.\"\"\"\n",
        "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', DecisionTreeRegressor(max_depth=10, random_state=42))])\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def train_knn(X_train, y_train, preprocessor):\n",
        "    \"\"\"Trenuje model k-Najbliższych Sąsiadów.\"\"\"\n",
        "    model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('regressor', KNeighborsRegressor(n_neighbors=5))])\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# --- 3. EWALUACJA ---\n",
        "def evaluate_model(model, X_test, y_test_log, model_name, ax=None):\n",
        "    \"\"\"Dokonuje predykcji, liczy metryki i opcjonalnie rysuje wykres.\"\"\"\n",
        "    # Predykcja\n",
        "    y_pred_log = model.predict(X_test)\n",
        "    \n",
        "    # Odwrócenie logarytmu (powrót do PLN)\n",
        "    y_test_true = np.expm1(y_test_log)\n",
        "    y_pred_true = np.expm1(y_pred_log)\n",
        "    \n",
        "    # Metryki\n",
        "    mae = mean_absolute_error(y_test_true, y_pred_true)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_true, y_pred_true))\n",
        "    r2 = r2_score(y_test_true, y_pred_true)\n",
        "    \n",
        "    # Wizualizacja\n",
        "    if ax:\n",
        "        sns.scatterplot(x=y_test_true, y=y_pred_true, alpha=0.3, ax=ax)\n",
        "        max_val = max(y_test_true.max(), y_pred_true.max())\n",
        "        ax.plot([0, max_val], [0, max_val], 'r--') # Idealna linia\n",
        "        ax.set_title(f\"{model_name}\\nR² = {r2:.3f}\")\n",
        "        ax.set_xlabel(\"Rzeczywista Cena\")\n",
        "        ax.set_ylabel(\"Przewidywana Cena\")\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'MAE (zł)': mae,\n",
        "        'RMSE (zł)': rmse,\n",
        "        'R²': r2\n",
        "    }\n",
        "\n",
        "# --- 4. GŁÓWNA FUNKCJA STERUJĄCA ---\n",
        "def run_ml_experiments(df, title, city_filter=None, exclude_city=None):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"URUCHAMIAM EKSPERYMENTY: {title}\")\n",
        "    if city_filter: print(f\"Filtr miasta: {city_filter}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # 1. Przygotowanie danych\n",
        "    X_train, X_test, y_train, y_test, num_feats, cat_feats = prepare_data(df, city_filter, exclude_city)\n",
        "    \n",
        "    # 2. Przygotowanie preprocessora\n",
        "    preprocessor = get_preprocessor(num_feats, cat_feats)\n",
        "    \n",
        "    # 3. Trening modeli (każdy osobną funkcją)\n",
        "    models = [\n",
        "        ('Regresja Liniowa', train_linear_regression(X_train, y_train, preprocessor)),\n",
        "        ('Drzewo Decyzyjne', train_decision_tree(X_train, y_train, preprocessor)),\n",
        "        ('k-NN (k=5)', train_knn(X_train, y_train, preprocessor))\n",
        "    ]\n",
        "    \n",
        "    # 4. Ewaluacja i Raportowanie\n",
        "    results = []\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    \n",
        "    for i, (name, model) in enumerate(models):\n",
        "        ax = plt.subplot(1, 3, i+1)\n",
        "        metrics = evaluate_model(model, X_test, y_test, name, ax)\n",
        "        results.append(metrics)\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Tabela wyników\n",
        "    results_df = pd.DataFrame(results)\n",
        "    display(results_df.style.highlight_max(axis=0, subset=['R²'], color='green')\n",
        "                      .highlight_min(axis=0, subset=['MAE (zł)', 'RMSE (zł)'], color='green')\n",
        "                      .format(\"{:.2f}\", subset=['MAE (zł)', 'RMSE (zł)'])\n",
        "                      .format(\"{:.3f}\", subset=['R²']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23184d43",
      "metadata": {},
      "outputs": [],
      "source": [
        "run_ml_experiments(df_sell, \"RYNEK SPRZEDAŻY (Cała Polska)\")\n",
        "\n",
        "run_ml_experiments(df_sell, \"RYNEK SPRZEDAŻY (Tylko Warszawa)\", city_filter=\"warszawa\")\n",
        "\n",
        "run_ml_experiments(df_sell, \"RYNEK SPRZEDAŻY (Cała Polska bez Warszawy)\", exclude_city=\"warszawa\")\n",
        "\n",
        "run_ml_experiments(df_rent, \"RYNEK SPRZEDAŻY (Cała Polska)\")\n",
        "\n",
        "run_ml_experiments(df_rent, \"RYNEK SPRZEDAŻY (Tylko Warszawa)\", city_filter=\"warszawa\")\n",
        "\n",
        "run_ml_experiments(df_rent, \"RYNEK SPRZEDAŻY (Cała Polska bez Warszawy)\", exclude_city=\"warszawa\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49a3e51",
      "metadata": {},
      "source": [
        "## 7. Wnioski i interpretacja wyników modelowania\n",
        "\n",
        "Na podstawie przeprowadzonych eksperymentów i analizy metryk błędów (MAE, RMSE, $R^2$), sformułowano następujące wnioski dotyczące skuteczności algorytmów w predykcji cen nieruchomości:\n",
        "\n",
        "1.  **Dominacja algorytmu k-Najbliższych Sąsiadów (k-NN):**\n",
        "    Najlepsze wyniki predykcyjne (najwyższy współczynnik $R^2$ oraz najniższe błędy MAE/RMSE) osiągnął model **k-NN**. Wynik ten ma silne uzasadnienie merytoryczne:\n",
        "    * **Natura rynku:** Wycena nieruchomości w praktyce opiera się na tzw. *podejściu porównawczym* (analiza transakcji podobnych lokali w okolicy). Algorytm k-NN działa w sposób analogiczny – estymuje cenę na podstawie średniej z $k$ najbardziej zbliżonych punktów w wielowymiarowej przestrzeni cech.\n",
        "    * **Lokalność:** Dzięki uwzględnieniu cech takich jak `city` oraz `centreDistance`, algorytm skutecznie znajduje sąsiadów o podobnej charakterystyce lokalizacyjnej, co jest kluczowe dla ceny.\n",
        "\n",
        "2.  **Znaczenie skalowania danych:**\n",
        "    Wysoka skuteczność k-NN potwierdza, że zastosowany **StandardScaler** został użyty poprawnie. Ponieważ k-NN opiera się na obliczaniu odległości euklidesowych, sprowadzenie metrażu (rzędu 50 m²) i odległości od centrum (rzędu 5 km) do wspólnej skali było krytyczne dla sukcesu tego modelu.\n",
        "\n",
        "3.  **Ograniczenia Regresji Liniowej:**\n",
        "    Model regresji liniowej osiągnął słabsze wyniki w porównaniu do k-NN. Wskazuje to, że zależności na rynku nieruchomości nie są w pełni liniowe. Przykładowo, wpływ odległości od centrum na cenę nie jest stały (cena spada gwałtownie blisko centrum i stabilizuje się na peryferiach), co dla prostej regresji jest trudne do odwzorowania bez zaawansowanej inżynierii cech.\n",
        "\n",
        "4.  **Wydajność Drzewa Decyzyjnego:**\n",
        "    Drzewo decyzyjne uplasowało się zazwyczaj pośrodku stawki (lub blisko k-NN). Choć dobrze radzi sobie z nieliniowością, może mieć tendencję do \"schodkowania\" predykcji (przypisywania tej samej ceny dla grupy mieszkań w jednym liściu), podczas gdy k-NN oferuje bardziej płynną interpolację cen, co w przypadku zmiennej ciągłej (ceny) daje mniejszy błąd średni.\n",
        "\n",
        "**Podsumowując:** Eksperyment wykazał, że dla tego zbioru danych podejście oparte na podobieństwie (k-NN) jest skuteczniejsze niż podejście oparte na regułach (Drzewa) lub prostych równaniach liniowych (Regresja)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
